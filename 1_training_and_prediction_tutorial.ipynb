{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a02ba23-bf45-4190-b33e-bb6ea78eb684",
   "metadata": {},
   "source": [
    "# Notebook 1) Skyrmion U-Net training & first prediction tutorial\n",
    "\n",
    "This notebook demonstrates how to define a neural network using a mini U-Net model, as well as how to train it, make predictions, and benchmark its performance. All the methods required throughout this workflow are introduced. \n",
    "\n",
    "For training the mini U-Net model, a small dataset is used, making it feasible to train on a CPU within a reasonable timeframe. After that, the process of training a larger U-Net will be shown; the approach remains the same, with only certain parameters increased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265f9bf-8fcd-44a3-95b7-7a34e62b559c",
   "metadata": {},
   "source": [
    "## 0. Configure notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224fabf-49ec-42b4-b905-c3cdf77d2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "    ![ ! -d \"AI-Magnetism-Session-Regensburg-2025\" ] ||  [ ! -d \"AI-Magnetism-Session-Regensburg-2025/.git\" ] && git clone https://github.com/kfjml/AI-Magnetism-Session-Regensburg-2025\n",
    "    ! pip install \"numba>=0.61.0,<0.62\" \"tensorflow[and-cuda]>=2.16.2,<3\" \"albumentations>=2.0.4,<3\"  \"pandas>=2.2.2,<3\" \"chardet>=5.2.0,<6\" \"opencv-python-headless>=4.11.0.86,<5\" \"wget>=3.2,<4\" \"pyyaml>=6.0.2, <7\" \"pillow>=11.1.0, <12\"\n",
    "    ! pip install \"ipympl>=0.9.6\" \"ipywidgets>=7.7.1\" \"matplotlib>=3.10.0,<4\"\n",
    "    basis_dir = \"/content/AI-Magnetism-Session-Regensburg-2025/\"\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except:\n",
    "    basis_dir = \"./\"\n",
    "    in_colab = False\n",
    "\n",
    "mini_unet_loaded_from_repository = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4946b69-c41c-4be0-bf19-5d1cb5c746b0",
   "metadata": {},
   "source": [
    "### If you are running this notebook in **Google Colab**, after executing the first cell (cell above), go to **Runtime → Restart session**, then rerun the first cell. After that, you can execute the cells below. This is necessary because some required packages are installed in Google Colab and need a restart to take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151bc305-0b48-4991-a435-5223af45bbb1",
   "metadata": {},
   "source": [
    "## 1. Import packages\n",
    "\n",
    "In this notebook, the key packages used are **TensorFlow** and **Keras**, essential for building, training, and performing inference with deep learning models efficiently. TensorFlow provides a robust platform for machine learning, while Keras simplifies model development with its user-friendly API. Additional packages are included to create an ecosystem that enables both training and inference processes. Additionally, the following packages are used:\n",
    "- `albumentations` - Image augmentation library for computer vision.  \n",
    "- `cv2 (OpenCV)` - Used here only in combination with `albumentations`.  \n",
    "- `PIL (Pillow)` - Image processing library.  \n",
    "- `numpy` - Numerical computing library for tensors.  \n",
    "- `matplotlib.pyplot` - Visualization library for plotting.  \n",
    "- `pandas` - Data (table) manipulation and analysis library.  \n",
    "- `glob` - File path management tool.  \n",
    "- `os` - Interface for file/folder creation, existence checks, filename handling, and more.  \n",
    "- `time` - Used to measure execution time, among other functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01d74c-eb00-40d4-a5cf-f0ef10c25d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import albumentations\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7394082-2e8c-4a68-8dd7-21a421cee4ee",
   "metadata": {},
   "source": [
    "### 1.1. Check if GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181db369-c3ae-409d-81d6-1b84a16a993e",
   "metadata": {},
   "source": [
    "If a GPU is available, we use the float16 policy for better performance, as it allows for faster computation and reduced memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34393e4-757a-4ff2-a1b9-36b4aafbb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that returns true if a GPU is available\n",
    "gpu_available = lambda : len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "#in the case of GPU, switch to mixed_float16 policy\n",
    "if gpu_available():\n",
    "    print(\"GPU is available\")\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70a39c-4591-4e6c-8e4e-c03af2a2719f",
   "metadata": {},
   "source": [
    "## 1.2 Load Dataset Table\n",
    "\n",
    "Load the dataset table containing the **Kerr micrograph images** and **segmentation mask labels**, which are the ground truth. A **segmentation mask** is an image where each pixel is assigned a class label, represented using different colors, to highlight distinct regions and specify how each pixel should be classified into a particular class.\n",
    "\n",
    "In machine learning, **ground truth** is the benchmark used to evaluate or train a model. \n",
    "\n",
    "Additionally, the same **source ID** indicates that the images originate from the same video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f74cb-aead-460b-8ed7-8cb17fe5b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_table = pd.read_csv(basis_dir+\"dataset/table.csv\",sep=\";\")\n",
    "dataset_table[\"img_fn\"] = dataset_table.apply(lambda x:basis_dir+x[\"img_fn\"],axis=1)\n",
    "dataset_table[\"label_fn\"] = dataset_table.apply(lambda x:basis_dir+x[\"label_fn\"],axis=1)\n",
    "dataset_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6071c-0176-4dd2-9b3f-be311d233677",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Load of an example of this dataset. The first dimension/axis of the array is the height, the second is the width, and in the case of the Kerr micrographs, the third is the RGB channel. In the case of grayscale images (the labels are color-coded, while the Kerr micrographs are black and white), there are no multiple channels, so there is no fourth axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612e182-e4e7-4e9b-b7ff-432661daa503",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 110\n",
    "kerr_micrograph = np.array(Image.open(dataset_table.loc[ix][\"img_fn\"]))\n",
    "print(\"kerr_micrograph\\n\",kerr_micrograph)\n",
    "segmentation_mask_label = np.array(Image.open(dataset_table.loc[ix][\"label_fn\"]))\n",
    "print(\"segmentation_mask_label\\n\",segmentation_mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350fadc2-c90b-4003-8c8c-b37113ab6d7b",
   "metadata": {},
   "source": [
    "Plot of this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40535fe4-4935-4fb4-b2ae-6b36e3aff694",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=2,dpi=300,figsize=(10,10))\n",
    "ax[0].imshow(kerr_micrograph,cmap=\"grey\")\n",
    "ax[0].set_title(\"Kerr micrograph\")\n",
    "ax[1].imshow(segmentation_mask_label)\n",
    "ax[1].set_title(\"Groundtruth\\n\"+\"Segmentation mask label\")\n",
    "ax[0].set_xlabel(\"x [px]\")\n",
    "ax[1].set_xlabel(\"x [px]\")\n",
    "ax[0].set_ylabel(\"y [px]\")\n",
    "ax[1].set_ylabel(\"y [px]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330624f2-ba3f-4d63-b42b-3856694a632d",
   "metadata": {},
   "source": [
    "With the segmentation mask for each Kerr microscopy image, it becomes easy to identify the objects (such as skyrmions) and determine their geometric properties, providing a basis for future analysis. That's why we aim to **train** this segmentation mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a2280-9cd0-4d9c-90a7-b5516e9e752e",
   "metadata": {},
   "source": [
    "## 1.3 Segmentation Mask Label\n",
    "\n",
    "The Kerr micrographs are labeled with a segmentation mask. The segmentation mask in the dataset consists of five distinct classes:\n",
    "\n",
    "- **Skyrmions** — RGB label: red [255, 0, 0]  \n",
    "- **Defects** — RGB label: green [0, 255, 0]  \n",
    "- **Ferromagnetic (FM) background** — RGB label: blue [0, 0, 255]  \n",
    "- **Non-Ferromagnetic (FM) background** — RGB label: yellow [255, 255, 0]  \n",
    "- **Boundary non-Ferromagnetic/Ferromagnetic background** — RGB label: cyan [0, 255, 255]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d099a3-ac4d-42c1-a053-8086da27fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = [(\"Skyrmions\",(1,0,0)),(\"Defects\",(0,1,0)),(\"Ferromagnetic (FM) background\",(0,0,1)),\n",
    "               (\"Non-Ferromagnetic (FM) background\",(1,1,0)),(\"Boundary non-Ferromagnetic/Ferromagnetic background\",(0,1,1))]\n",
    "fig,ax = plt.subplots(figsize=(5, 2),dpi=130,facecolor=\"k\")\n",
    "for i, (label, color) in enumerate(text_labels):\n",
    "    ax.text(0.5, 1 - i * 0.2, label, fontsize=15, color=color, ha='center')\n",
    "ax.set_ylim(0,1.2)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"U-Net classes\",fontsize=20,color=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d34f0-3d0d-4f2a-830c-dec7c9a3ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=2,dpi=300,figsize=(12,3.2))\n",
    "ax[0].imshow(kerr_micrograph,cmap=\"grey\")\n",
    "ax[0].set_title(\"Kerr micrograph\")\n",
    "ax[1].imshow(segmentation_mask_label)\n",
    "ax[1].set_title(\"Groundtruth\\n\"+\"Segmentation mask label\")\n",
    "ax[0].set_xlabel(\"x [px]\")\n",
    "ax[1].set_xlabel(\"x [px]\")\n",
    "ax[0].set_ylabel(\"y [px]\")\n",
    "ax[1].set_ylabel(\"y [px]\")\n",
    "\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3,4,5],6), \n",
    "                    cmap=matplotlib.colors.ListedColormap([(0, 1, 1), (1, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0)])),\n",
    "                    ax=ax[1], ticks=[0.5, 1.5, 2.5,3.5,4.5])\n",
    "cbar.set_ticklabels(['FM-non FM Boundary', 'non FM Bakckground', 'FM Bakckground', 'Defects', 'Skyrmions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e16be-7aff-4314-bc0a-7287ed4582d7",
   "metadata": {},
   "source": [
    "We don't want to train the 5 classes, but we aim to train a 3 classes. Furthermore, instead of the RGB labels, the classes are assigned a class index. \n",
    "\n",
    "The following classes are used:\n",
    "\n",
    "- **Skyrmions** - Index 0, RGB label: red [255, 0, 0]  \n",
    "- **Defects** - Index 1, RGB label: green [0, 255, 0]\n",
    "- **Background** - Index 2, RGB label: blue [0, 0, 255]  \n",
    "   - The background class includes:\n",
    "     - The ferromagnetic (FM) background  \n",
    "     - The non-ferromagnetic (non-FM) background  \n",
    "     - The boundary between ferromagnetic and non-ferromagnetic backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a9280-383c-4641-ae56-37f5276c4b8f",
   "metadata": {},
   "source": [
    "Let's now define the conversion function from RGB index to class index and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ddd17-4c97-405f-807b-8f723c54e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trafo_rgb_to_class(I):\n",
    "    Q = np.zeros((I.shape[0],I.shape[1]),dtype=np.uint8)\n",
    "    R,G,B = I[:,:,0],I[:,:,1],I[:,:,2]\n",
    "    skyrmion_mask = (R>=128)&(G<128)&(B<128)\n",
    "    defect_mask = (R<128)&(G>=128)&(B<128)\n",
    "    bck_mask = ~(skyrmion_mask|defect_mask)\n",
    "    Q[skyrmion_mask] = 0\n",
    "    Q[defect_mask] = 1\n",
    "    Q[bck_mask] = 2\n",
    "    return Q\n",
    "\n",
    "def trafo_class_to_rgb(I):\n",
    "    basis = np.array([[255,0,0],[0,255,0],[0,0,255],[255,255,0],[0,255,255]],dtype=np.uint8)\n",
    "    return basis[I]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce27030-ad51-492f-87d6-0f75e048fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"segmentation_mask_label.shape\",segmentation_mask_label.shape)\n",
    "print(\"segmentation_mask_label=\\n\",segmentation_mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c638c-ebb2-412d-840b-dca517021f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_mask_label_class = trafo_rgb_to_class(segmentation_mask_label)\n",
    "print(\"segmentation_mask_label_class.shape\",segmentation_mask_label_class.shape)\n",
    "print(\"segmentation_mask_label_class=\\n\",segmentation_mask_label_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f7fad-1808-4167-afe3-da32c9ae90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,dpi=300,figsize=(10.4,2.4),constrained_layout=True)\n",
    "ax[0].imshow(kerr_micrograph,cmap=\"gray\")\n",
    "ax[2].imshow(trafo_class_to_rgb(trafo_rgb_to_class(segmentation_mask_label)))\n",
    "im = ax[1].imshow(segmentation_mask_label_class,cmap=\"gray\")\n",
    "col = fig.colorbar(im,ax=ax[1],label=\"class\")\n",
    "col.set_ticks([0,1,2])\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3],3), \n",
    "                    cmap=matplotlib.colors.ListedColormap([(0,0,1),(0,1,0),(1,0,0)])),\n",
    "                    ax=ax[2], ticks=[0.5, 1.5, 2.5])\n",
    "cbar.set_ticklabels(['Background', 'Defects', 'Skyrmions'])\n",
    "ax[0].set_title(\"Kerr micrograph\")\n",
    "ax[2].set_title(\"Groundtruth\\n\"+\"Mask (RGB code for the 3 classes)\")\n",
    "ax[1].set_title(\"Groundtruth\\n\"+\"Mask (Class index for the 3 classes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6965f2-88e4-4de4-b91d-444964dcb100",
   "metadata": {},
   "source": [
    "## 2. Define U-Net architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd195e-3e56-4134-80d2-fbec0dc6eb68",
   "metadata": {},
   "source": [
    "We now want to build this U-Net architecture step by step while understanding the different components involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e9804-bee8-4b7a-b585-e0925b219cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define plot function for figures\n",
    "def plotfig(fn,dpi):\n",
    "    fig,ax = plt.subplots(dpi=dpi)\n",
    "    ax.imshow(plt.imread(fn))\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "plotfig(basis_dir+\"notebook_figures/u_net_architecture_2.png\",320)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2082c0-1b2a-43da-ad9b-0fcca723aae8",
   "metadata": {},
   "source": [
    "### 2.1. U-Net Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b4af3-b0d6-4503-badf-e799054c56cb",
   "metadata": {},
   "source": [
    "First, we define some plotting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df547d-45dc-46ba-addc-7f8a2801f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotArray(ax,A,plot_value=True):\n",
    "    im = ax.imshow(A,cmap=\"gray\",origin=\"lower\")\n",
    "    fig.colorbar(im,ax=ax)\n",
    "    ax.set_ylabel(\"2. axis/dimension\\n (y-axis of the image)\")\n",
    "    ax.set_xlabel(\"3. axis/dimension\\n (x-axis of the image)\")\n",
    "    if plot_value:\n",
    "        for i in range(A.shape[0]):\n",
    "            for j in range(A.shape[1]):\n",
    "                ax.text(j,i, f'{A[i, j]:.2f}', ha='center', va='center',\n",
    "            color='white' if (A[i, j]-np.min(A))/(np.max(A)-np.min(A)) < 0.5 else 'black')\n",
    "\n",
    "    ax.set_xticks(np.arange(A.shape[1]))\n",
    "    ax.set_yticks(np.arange(A.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9947a6-404e-4e2d-a9be-e5e8901583ec",
   "metadata": {},
   "source": [
    "#### 2.1.1 Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74003390-d383-4d3f-b5c8-89af26e2b68d",
   "metadata": {},
   "source": [
    "We will use tensors in the following. Tensors (for this workshop) are multi-dimensional arrays that represent data in various shapes and types.\n",
    "\n",
    "For our tensors $T_{i,x,y,c}$, the first dimension/axis represents the image index (size: batch size), the second corresponds to the y-axis (size: height), the third to the x-axis (size: width), and the fourth represents the channels per pixel (size: number of channels per pixel). For example, each channel may represent the probability that a pixel belongs to a specific class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327daa6d-fda7-4e45-bfc5-d12b09995313",
   "metadata": {},
   "source": [
    "An example with 4 images of size 5x4, each with 4 channels, containing values generated from Gaussian random numbers. Then look at the 3rd channel of the 2nd image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44722ba-c17b-435d-8603-86c4bc55980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(4,5,4,4)\n",
    "fig,ax = plt.subplots(dpi=100)\n",
    "PlotArray(ax,A[1,:,:,2])\n",
    "ax.set_title(\"1st axis = 1 (second image),\\n4th axis = 2 (third channel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7814e4bc-fbf0-4a3d-87c4-01d1a320cd2e",
   "metadata": {},
   "source": [
    "A tensor is also the set of Kerr micrograph images stored in our repository. These can also be represented as a tensor with the shape (number of Skyrmion images, width, height, channel index). Additionally, we normalize the data between 0 and 1 for training and prediction. Normalization is very important for inputs and the layers in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0a04f-b022-41bf-82b8-b14a1b967ce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"kerr_micrograph.shape\",kerr_micrograph.shape)\n",
    "kerr_micrograph_tensor = kerr_micrograph[None,:,:,None]/255\n",
    "print(\"kerr_micrograph_tensor.shape\",kerr_micrograph_tensor.shape)\n",
    "kerr_micrograph_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525aaf43-1607-41bf-951f-9205214db2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(dpi=200)\n",
    "im = ax.imshow(kerr_micrograph_tensor[0,:,:,0],cmap=\"gray\")\n",
    "fig.colorbar(im)\n",
    "ax.set_ylabel(\"2. axis/dimension\")\n",
    "ax.set_xlabel(\"3. axis/dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef574eb-0d79-419c-b95e-460f9fdafbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a409680-5267-4332-a41f-31545e32f67b",
   "metadata": {},
   "source": [
    "#### 2.1.2 Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d2c4e-9d6d-421f-8378-12b05b93deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/convolution_layer.png\",120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c72fa-d2dc-4b1b-ab14-0b5c75441b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/conv_1.png\",300)\n",
    "plotfig(basis_dir+\"notebook_figures/conv_2.png\",300)\n",
    "plotfig(basis_dir+\"notebook_figures/conv_3.png\",300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c789a7-047b-4487-ac3f-ed818b7949b9",
   "metadata": {},
   "source": [
    "An important layer that does have weights is the **Convolution Layer**, which performs a convolution — a linear operation. The weights in this layer are learned during training. The convolutional layer implemented in TensorFlow using `Conv2D` consists of a convolution and a bias layer. The kernel is a small tensor that acts as a filter with defined dimensions, moving across the input and computing dot products with overlapping regions, effectively performing the convolution. Notably, TensorFlow's `Conv2D` automatically flips the kernel by 180 degrees (both horizontally and vertically) before performing the convolution, ensuring it matches the mathematical definition of convolution. A bias term is then added to the result of each dot product. The kernel size defines the dimensions of this filter, determining the region of the input considered for each output value. With `padding='same'`, zero-padding is added around the input so that the output size matches the input size when the stride is 1. The number of padding pixels added depends on the kernel size and input dimensions.\n",
    "\n",
    "Convolution can be written using the following formula:  \n",
    "\n",
    "$$B_{i,x,y,c} = C_c + \\sum_{m,n,h} \\mathcal{K}_{m,n,h,c} A_{i,x-m,y-n,h}$$\n",
    "\n",
    "where $ A $ is the input tensor and $ B $ is the output tensor, both of which are 4-dimensional. The $ \\mathcal{K} $ denotes the kernel, and $ C_c $ is a channel-specific offset (or bias) that is added. .\n",
    "\n",
    "Let's now create an example where we set the weights. The first two axes describe the kernel along the x and y axes of the image, while the third and fourth axes are for treatment with multiple channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255a863-0616-40e4-8318-b93fb472c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvolutionalLayer = tf.keras.layers.Conv2D(2,kernel_size=(3,3),padding=\"same\")\n",
    "# initialize convolutional layer size with dummy input\n",
    "ConvolutionalLayer(np.random.randn(1,6,6,4))\n",
    "ConvolutionalLayer.get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032acce8-41e4-4928-b65c-e9e7009a6d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215f42e-b809-4837-b9c9-369c45144eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvolutionalLayer = tf.keras.layers.Conv2D(1,kernel_size=(3,3),padding=\"same\")\n",
    "# initialize convolutional layer size with dummy input\n",
    "ConvolutionalLayer(np.random.randn(1,6,6,1))\n",
    "print(\"Weights shape:\",ConvolutionalLayer.get_weights()[0].shape)\n",
    "print(\"Bias shape:\",ConvolutionalLayer.get_weights()[1].shape)\n",
    "\n",
    "kernel = np.array([[[[0]],[[0.5]],[[0]]],\n",
    "                   [[[1]],[[0]],[[1]]],\n",
    "                   [[[1]],[[0]],[[0]]]])\n",
    "bias = np.array([10])\n",
    "\n",
    "ConvolutionalLayer.set_weights([kernel,bias])\n",
    "\n",
    "print(\"Weights:\\n\",ConvolutionalLayer.get_weights()[0])\n",
    "print(\"Bias:\\n\",ConvolutionalLayer.get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e4571-ee56-41fa-9f94-70239545c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,3.5),ncols=2,dpi=200)\n",
    "PlotArray(ax[0],kernel[:,:,0,0],False)\n",
    "PlotArray(ax[1],kernel[::-1,::-1,0,0],False)\n",
    "ax[0].set_title(\"Kernel\")\n",
    "ax[0].set_ylabel(\"1. axis/dimension\\n (acts on y-axis of the image)\")\n",
    "ax[0].set_xlabel(\"2. axis/dimension\\n (acts on x-axis of the image)\")\n",
    "ax[1].set_title(\"Kernel\\nhorizontally & vertically\\nflipped\")\n",
    "ax[1].set_ylabel(\"1. axis/dimension\\n (acts on y-axis of the image)\")\n",
    "ax[1].set_xlabel(\"2. axis/dimension\\n (acts on x-axis of the image)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aebc99c-2dc6-4687-8b79-49ba13f68b03",
   "metadata": {},
   "source": [
    "We use in the following example this kernel, and after the dot product, bias 10 is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a6e95-1212-4586-8f4b-7a76ac276622",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((1,8,8,1))\n",
    "A[0,2,2] = 10\n",
    "A[0,5,5] = 10\n",
    "A[0,2,2] = 10\n",
    "A[0,1,6] = 10\n",
    "A[0,1,7] = 10\n",
    "B = ConvolutionalLayer(A)\n",
    "fig,ax = plt.subplots(ncols=2,figsize=(6,2.5),dpi=200)\n",
    "PlotArray(ax[0],A[0,:,:,0],False)\n",
    "PlotArray(ax[1],B[0,:,:,0],False)\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Output\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b022acec-17f3-428f-9827-be81487bdd52",
   "metadata": {},
   "source": [
    "Now, a further example: a larger kernel size with the weights set to a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c337c23b-39c2-419b-b052-8f441f508477",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = kerr_micrograph_tensor.copy()\n",
    "n_kernel = 20\n",
    "ConvolutionalLayer = tf.keras.layers.Conv2D(1,kernel_size=(n_kernel,n_kernel),padding=\"same\")\n",
    "# initialize convolutional layer size with dummy input\n",
    "ConvolutionalLayer(A)\n",
    "\n",
    "x,y = np.meshgrid(np.arange(n_kernel)-(n_kernel-1)/2,np.arange(n_kernel)-(n_kernel-1)/2)\n",
    "sigma = 4\n",
    "K = (np.exp(-(x**2+y**2)/(2*sigma**2)))\n",
    "K = K.reshape((n_kernel,n_kernel,1,1))\n",
    "ConvolutionalLayer.set_weights([K,np.array([0.0])])\n",
    "\n",
    "print(\"Weights shape:\",ConvolutionalLayer.get_weights()[0].shape)\n",
    "print(\"Bias shape:\",ConvolutionalLayer.get_weights()[1].shape)\n",
    "B = ConvolutionalLayer(A)\n",
    "fig,ax = plt.subplots(ncols=2,dpi=300)\n",
    "ax[0].imshow(A[0,:,:,0],cmap=\"gray\")\n",
    "ax[1].imshow(B[0,:,:,0],cmap=\"gray\")\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca2109a-694c-45af-97ab-4eccfeec7506",
   "metadata": {},
   "source": [
    "Now we have an implementation of a Gaussian blur in the form of a tensor transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd38b6a-bf61-4007-875e-7bd96fd5af3e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d49ce-200f-4ac0-aa0d-95db3dcdb977",
   "metadata": {},
   "source": [
    "For the **initialization** of the weights **for training**, we use He Normalization. In this method, the weights are initialized with values drawn from a normal distribution with a mean of zero and a variance that is scaled based on the number of input units. This helps improve convergence and stability.\n",
    "\n",
    "Now an initialization of the convolutional layer with He normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a414ec-a1da-4c81-8cfc-fa852be1687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = kerr_micrograph_tensor.copy()/255\n",
    "ConvolutionalLayer = tf.keras.layers.Conv2D(1,kernel_size=(3,3),padding=\"same\",kernel_initializer=\"he_normal\")\n",
    "# initialize convolutional layer size with dummy input\n",
    "ConvolutionalLayer(A)\n",
    "print(\"Weights shape:\",ConvolutionalLayer.get_weights()[0].shape)\n",
    "print(\"Bias shape:\",ConvolutionalLayer.get_weights()[1].shape)\n",
    "\n",
    "B = ConvolutionalLayer(A)\n",
    "fig,ax = plt.subplots(ncols=2,dpi=300)\n",
    "ax[0].imshow(A[0,:,:,0],cmap=\"gray\")\n",
    "ax[1].imshow(B[0,:,:,0],cmap=\"gray\")\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0da2ee5-a9aa-428d-9458-efb53ab96e34",
   "metadata": {},
   "source": [
    "Here we have now looked at where the input and output are in the channel. In general, what we use in the following applies. In `tf.keras.layers.Conv2D`, the first argument defines the number of output channels, regardless of the number of input channels. Each output channel is generated by applying a separate filter to all input channels. The resulting value at each position is calculated as the sum of the element-wise products between the filter's weights and the corresponding input values across all channels. Consequently, the number of output channels is exactly equal to the value of the first argument. It makes sense to have more output channels than input channels because this way different convolutions can be applied to an image, allowing multiple features or properties to be extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2e95b-7b46-4f0c-a926-d4df16dc4e5a",
   "metadata": {},
   "source": [
    "Now an example with multiple channels for input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a72e5-50f1-4132-bbc8-9ef483bb74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvolutionalLayer = tf.keras.layers.Conv2D(5,kernel_size=(3,3))\n",
    "A = np.random.randn(5,20,20,2)\n",
    "print(\"Input shape:\",A.shape)\n",
    "print(\"Output shape:\",ConvolutionalLayer(A).shape)\n",
    "print(\"Weight/Convolution shape:\",ConvolutionalLayer.get_weights()[0].shape)\n",
    "print(\"Bias shape:\",ConvolutionalLayer.get_weights()[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5755fe-69d3-49e0-a9dd-48df9c66cf89",
   "metadata": {},
   "source": [
    "#### 2.1.3 Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6f2ba-3415-426e-a158-c415b2c83e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/activation_layer.png\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530904c-24e8-44af-9e7a-d169179484f9",
   "metadata": {},
   "source": [
    "Activation functions are non-linear and crucial for neural networks, enabling them to learn complex patterns and improve performance by introducing non-linear decision boundaries. Without these non-linearities, neural networks would be linear and far less capable of handling complex patterns and data relationships.\n",
    "\n",
    "In the following we use the Mish function. The Mish function is defined as follows:\n",
    "\n",
    "$$f(x) = x \\cdot \\tanh(\\log(1 + e^x))$$\n",
    "\n",
    "The Activation Layer only performs a function and usually does not contain any weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ad62b-32b6-4d40-876a-e320d2808096",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(3,3),dpi=100)\n",
    "x = np.linspace(-10,10)\n",
    "ax.plot(x,x*np.tanh(np.log(1+np.exp(x))))\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Mish function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144828a9-b54b-434a-bdc2-8329234f40a6",
   "metadata": {},
   "source": [
    "Activation functions are usually combined with e.h. convolution layers; however, here we will focus first solely on the activation function as a standalone layer.\n",
    "\n",
    "We will now observe what happens when the Mish function is applied to a tensor filled with random numbers. Of course, in practice, these won't be random numbers but rather the data that originates from the input, passes through various layers, and reaches this point in the neural network, where it then passes through the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e56b774-1d92-4705-a036-a2fef36295ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic activation layer\n",
    "class MishLayer(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.keras.activations.mish(x)\n",
    "\n",
    "#Define a Activation function as a layer\n",
    "ActivationFunction = MishLayer()\n",
    "A = np.random.randn(1,4,4,1)*10\n",
    "B = ActivationFunction(A)\n",
    "\n",
    "print(\"A=\\n\",A)\n",
    "print(\"B=\\n\",B)\n",
    "\n",
    "fig,ax = plt.subplots(ncols=2,figsize=(7.2,3),dpi=200)\n",
    "PlotArray(ax[0],A[0,:,:,0])\n",
    "PlotArray(ax[1],B[0,:,:,0])\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Output\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd366f-81ef-4daa-88d2-0c1493c4e975",
   "metadata": {},
   "source": [
    "#### 2.1.4 Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874eb27-d8b8-459b-b69e-cc6d4a7fa9fa",
   "metadata": {},
   "source": [
    "The `tf.keras.layers.BatchNormalization` layer is active only during training. It normalizes the input so that the mean becomes 0 and the standard deviation becomes 1, and then outputs the resulting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1ad5d-dd1b-45d4-9066-de565bba583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchNormalization = tf.keras.layers.BatchNormalization(momentum=1e-13)\n",
    "A = np.random.randn(1,10,10)*10+10\n",
    "B = BatchNormalization(A,training=True)\n",
    "print(\"A=\\n\",A)\n",
    "print(\"B=\\n\",B)\n",
    "\n",
    "print(\"mean A: \",np.mean(A),\"std B: \",np.std(A))\n",
    "print(\"mean B: \",np.mean(B),\"std B: \",np.std(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106545ea-7afb-4c0b-a3da-0d67d061be4f",
   "metadata": {},
   "source": [
    "The behavior of batch normalization is more complex than it may seem in this demo. During training, batch normalization does not calculate the mean and standard deviation for each individual input. Instead, it scales and shifts the data using running averages of the mean and standard deviation from previous batches. These moving averages are gradually updated over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c0439-6c40-455d-9cab-95f45a3281ad",
   "metadata": {},
   "source": [
    "#### 2.1.5 Convolutional block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e06bf8-b1cd-4b4f-b6d6-9420d7397c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/convolution_layer_w_activation_funtion.png\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d586811-2f4f-479a-8946-4613f8a9da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Convolution Block\n",
    "def conv_block(x, n_channels, param):\n",
    "    x = tf.keras.layers.Conv2D(n_channels, kernel_size=param[\"kernel_size\"],kernel_initializer=param[\"kernel_initialization\"],padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x) \n",
    "    x = MishLayer()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c747f9-48e1-400d-b668-b5c9782200ca",
   "metadata": {},
   "source": [
    "#### 2.1.5 Double Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c623c-bf3c-498b-a02e-59e832627a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/double_convolution_layer.png\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d216f-0d1f-47b5-b1c1-97b652e87cca",
   "metadata": {},
   "source": [
    "With this we build now a double convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6773512-b394-4c7e-862d-eae278183559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_channels, param):\n",
    "    x = conv_block(x,n_channels,param)\n",
    "    x = conv_block(x,n_channels,param)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cac0ab-01af-4be5-86f8-89879bc5a5f9",
   "metadata": {},
   "source": [
    "#### 2.1.6 Maximum Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b24967-60a6-4582-abeb-b9489f9bc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/max_pooling.png\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebcf9f-03a5-4d8f-ac44-c72391596e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/max_pooling_1.png\",300)\n",
    "plotfig(basis_dir+\"notebook_figures/max_pooling_2.png\",300)\n",
    "plotfig(basis_dir+\"notebook_figures/max_pooling_3.png\",300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37c8d1-edcb-499f-bb68-6f02ba4a150b",
   "metadata": {},
   "source": [
    "The TensorFlow Keras `MaxPooling2D` layer is used to reduce the spatial dimensions (height and width) of an input feature map while retaining important features. It works by applying a pooling operation, such as taking the maximum value from a defined window (e.g., 2x2) as it slides over the input. This process helps reduce the number of parameters, improve computational efficiency, and enhance model robustness by emphasizing the most prominent features in each region. Unlike convolution layers, MaxPooling has no learnable parameters; it simply downsamples the data based on the specified pool size and strides.\n",
    "\n",
    "Max pooling can be written using the following formula:\n",
    "\n",
    "$$B_{i, x, y, c} = \\max_{m, n} A_{i, s_x x + m, s_y y + n, c}$$\n",
    "\n",
    "where $ A $ is the input tensor and $ B $ is the output tensor, both of which are 4-dimensional. The terms $ s_x $ and $ s_y $ denote the stride in the horizontal and vertical directions, respectively. The maximum operation is performed over the spatial region defined by the pooling window. Unlike convolution, max pooling does not involve a kernel or additional parameters; it simply selects the maximum value within the defined window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4735201-b065-42ec-9eff-09dd9dd7d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxPool = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "A = np.random.randn(1,4,4,1)\n",
    "B = MaxPool(A)\n",
    "\n",
    "print(\"A=\\n\",A)\n",
    "print(\"B=\\n\",B)\n",
    "\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(ncols=2,figsize=(6,2.5),dpi=200)\n",
    "PlotArray(ax[0],A[0,:,:,0])\n",
    "PlotArray(ax[1],B[0,:,:,0])\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Output\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9be96d-b58f-4009-a5e3-be79c35ff01c",
   "metadata": {},
   "source": [
    "#### 2.1.7 Dropout Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ad6c2-e3af-45f2-8f2f-eae71077d27d",
   "metadata": {},
   "source": [
    "In `tf.keras.layers.Dropout`, a specific dropout rate (first argument) is set, which determines the fraction of input units to be randomly set to zero during training. This technique helps prevent **overfitting** by introducing regularization, making the learning process more robust and improving the model's generalization ability. **Overfitting** occurs when a machine learning model learns the noise and specific details of the training data too well, resulting in poor performance on new, unseen data. This happens because the model becomes too complex, capturing patterns that don't generalize beyond the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a507e-8bdc-47c6-b55c-2e2868c3707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout = tf.keras.layers.Dropout(0.4)\n",
    "A = np.random.randn(1,5,5,1)\n",
    "B = Dropout(A,training=True)\n",
    "\n",
    "print(\"A=\\n\",A)\n",
    "print(\"B=\\n\",B)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(ncols=2,figsize=(7.2,3),dpi=200)\n",
    "PlotArray(ax[0],A[0,:,:,0])\n",
    "PlotArray(ax[1],B[0,:,:,0])\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Output\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4ff8c-442e-47c1-b94f-146179e201a5",
   "metadata": {},
   "source": [
    "#### 2.1.8 Downsample block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a9486-c7b5-4ee2-a4eb-2cf0d53011ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/downsampling_block.png\",120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff30a3f-9626-4bae-99f3-1db22b48adee",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "This together gives the downsample block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42c284-450f-4d03-9cd1-183125a9603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_block(x, n_channels, param):\n",
    "    f = double_conv_block(x, n_channels, param)\n",
    "    p = tf.keras.layers.MaxPool2D(pool_size=(2,2))(f)\n",
    "    p = tf.keras.layers.Dropout(param[\"dropout\"])(p)\n",
    "    return f, p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd59768-3568-40f1-8951-7e7a426d38f5",
   "metadata": {},
   "source": [
    "#### 2.1.9 Upconvolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b17d62-327a-475d-a4b0-c91e0f7ea114",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/up_convolution.png\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924901e0-1eb4-41e1-94d0-15f7812d38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/up_conv_1.png\",300)\n",
    "plotfig(basis_dir+\"notebook_figures/up_conv_2.png\",300)\n",
    "plotfig(basis_dir+\"notebook_figures/up_conv_3.png\",300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac47ea-4daa-432a-83f8-7a26dbd464e7",
   "metadata": {},
   "source": [
    "Upconvolution is the opposite of max pooling. In upconvolution (also known as transposed convolution or deconvolution), interpolation is performed using a kernel for each pixel. Each pixel (in terms of width and height of an image) is multiplied by a kernel, effectively extrapolating the input to produce a larger output image. This process can be seen as the inverse of convolution: while convolution reduces the spatial dimensions by aggregating information from a larger region into a smaller one, upconvolution expands the spatial dimensions by distributing the value of each input pixel across a larger receptive field through the kernel.\n",
    "\n",
    "\n",
    "The concept of **strides** is crucial here. Strides determine the step size for moving the kernel across the image. Additionally, **padding** with `\"same\"` ensures that the final output size matches the original input size.\n",
    "\n",
    "The final number of output channels is specified in the first argument of the `Conv2DTranspose` function. This value defines the number of output feature maps, and the result is the sum over the input channels after the transposed convolution operation.\n",
    "\n",
    "\n",
    "The upconvolution (also known as transposed convolution or deconvolution) can be written using the following formula:\n",
    "\n",
    "$$B_{i, x, y, c} = \\sum_{m, n, h} \\mathcal{K}_{m, n, h, c} A_{i, \\left\\lfloor \\frac{x}{s_x} \\right\\rfloor + m, \\left\\lfloor \\frac{y}{s_y} \\right\\rfloor + n, h} + C_c$$\n",
    "\n",
    "where $ A $ is the input tensor and $ B $ is the output tensor, both of which are 4-dimensional. The symbol $ \\mathcal{K} $ denotes the kernel, and $ C $ is a channel-specific offset (or bias). The terms $ s_x $ and $ s_y $ denote the stride in the horizontal and vertical directions, respectively.   Unlike standard convolution, upconvolution effectively increases the spatial resolution by inserting values in between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462eb1de-f379-45f6-96b8-68bd9eb2d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UpConvolutionalLayer = tf.keras.layers.Conv2DTranspose(1,kernel_size=(2,2),strides=2,padding=\"same\")\n",
    "# initialize convolutional layer size with dummy input\n",
    "UpConvolutionalLayer(np.random.randn(1,3,3,1))\n",
    "print(\"Weights shape:\",UpConvolutionalLayer.get_weights()[0].shape)\n",
    "print(\"Bias shape:\",UpConvolutionalLayer.get_weights()[1].shape)\n",
    "\n",
    "Kernel = np.array([[[[0]],[[0.5]]],[[[1]],[[0]]]])\n",
    "Bias = np.array([-3])\n",
    "UpConvolutionalLayer.set_weights([Kernel,Bias])\n",
    "\n",
    "\n",
    "A = np.zeros((1,3,3,1))\n",
    "A[0,0,0] = 1\n",
    "A[0,1,2] = 0.5\n",
    "B = UpConvolutionalLayer(A)\n",
    "\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(4,3.5),dpi=200)\n",
    "PlotArray(ax,Kernel[:,:,0,0],False)\n",
    "ax.set_title(\"Kernel\")\n",
    "ax.set_ylabel(\"1. axis/dimension\\n (acts on y-axis of the image)\")\n",
    "ax.set_xlabel(\"2. axis/dimension\\n (acts on x-axis of the image)\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig,ax = plt.subplots(ncols=2,figsize=(6,2.5),dpi=200)\n",
    "PlotArray(ax[0],A[0,:,:,0],False)\n",
    "PlotArray(ax[1],B[0,:,:,0],False)\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Output\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458eff4-d43b-41e4-81f9-510f699e34ae",
   "metadata": {},
   "source": [
    "#### 2.1.10 Concatenation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f84a01-fd65-4fd9-b8b7-0f2fc2b5c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/concatenation_layer.png\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ca7e6-b5ae-4de8-8244-d576a3c5cb96",
   "metadata": {},
   "source": [
    "With the concatenation layer `tf.keras.layers.Concatenate`, you can concatenate tensors - the outputs of different layers - by merging their channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027df43a-fc85-497f-9130-660093b879f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = np.random.randn(1,2,2,2)\n",
    "input2 = np.random.randn(1,2,2,3)\n",
    "print(\"input1=\\n\",input1)\n",
    "print(\"input2=\\n\",input2)\n",
    "concat_layer = tf.keras.layers.Concatenate()\n",
    "output = concat_layer([input1, input2]) \n",
    "print(\"output=\\n\",output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952cce55-0d71-4742-bf6e-11f958c62e84",
   "metadata": {},
   "source": [
    "#### 2.1.11 Upsample block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3bbc7f-b4f6-4342-b6d2-a6fd3bfeac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/concatenation_2_layer.png\",120)\n",
    "plotfig(basis_dir+\"notebook_figures/upsampling_block.png\",120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44c30e-3820-492a-8c14-55285a87f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_block(x, conv_features, n_channels, param):\n",
    "    x = tf.keras.layers.Conv2DTranspose(n_channels*param[\"upsample_channel_multiplier\"], param[\"kernel_size\"], strides=(2,2), padding='same')(x)\n",
    "    x = tf.keras.layers.concatenate([x, conv_features])\n",
    "    x = tf.keras.layers.Dropout(param[\"dropout\"])(x)\n",
    "    x = double_conv_block(x, n_channels, param)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e512e-dd7a-42aa-801e-aca581689e02",
   "metadata": {},
   "source": [
    "### 2.2 Tensorflow models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc8d69-2115-4cbe-9721-b8abeb35f007",
   "metadata": {},
   "source": [
    "#### 2.2.1 Input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6b210-8d14-46b7-97f7-9751bf67ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/input_layer.png\",120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eaf1c5-e960-4e73-814d-a893121603e1",
   "metadata": {},
   "source": [
    "The input layer of a model is in tensorflow is defined as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764f2f8-f7a3-45cc-8069-137505e39d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input(shape=(9,9, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf39f2-4ae1-4c21-accb-4f9d5bca7a93",
   "metadata": {},
   "source": [
    "#### 2.2.2 Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f97f7-a9d3-4138-a697-adb1defe60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/segmentation_mask_output.png\",120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b00f2c-6991-48e1-956a-cf3675cb658f",
   "metadata": {},
   "source": [
    "We will look in the following at the following example's output layer. With the output layer being a convolutional layer with a $ (1, 1) $ kernel size that changes the channel size to 3 from another channel size. After that, the softmax function is applied to each pixel across the channel dimension (indexed by $i$) in the output tensor $\\mathbf{z}$, which has dimensions: image height, width, and number of channels. The softmax function converts $\\mathbf{z}$ to probabilities:\n",
    "\n",
    "$$p_{j,x,y,i} = \\frac{\\exp(\\mathbf{z}_{j,x,y,i})}{\\sum_{i \\in \\text{classes}} \\exp(\\mathbf{z}_{j,x,y,i})}$$\n",
    "\n",
    "The resulting probability tensor $p$ can then be converted to a class prediction mask $\\mathbf{m}_j$ by taking the index of the maximum probability for each pixel:\n",
    "\n",
    "$$ \\mathbf{m}_{j,x,y} = \\arg\\max_i \\; p_{j,x,y,i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcfc41-e70c-47ae-860d-909e11f94779",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "This layer can be implemented with `tf.keras.layers.Conv2D(3, (1, 1), padding=\"same\", activation=\"softmax\")`. \n",
    "\n",
    "And now an example where the weights of the convolution are set to the identity so that we can now observe the softmax behavior. And now an example where the weights of the convolution are set to the identity matrix so that we can now observe the softmax behavior. Therefore, we also set the input layer channels to 3; otherwise, the convolution layer would adjust them to 3 (as defined for the layer) with the convolution. As an example, we will make the prediction with an image that is one pixel wide and five pixels high, with three channels, to keep it simple. And now we plot the 2nd dimension of the tensor on the y-axis and the **channels** (4th axis, dimension) on the x-axis, not the 3rd dimension/axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d6b9ff-41f6-4655-9a88-c5d14610f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.keras.layers.Conv2D(3, (1,1),padding=\"same\",activation = \"softmax\")\n",
    "output_layer(np.random.randn(5,2,2,3))\n",
    "output_layer.set_weights([np.eye(3)[None,None],np.zeros(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a56a15-92fc-4fac-b908-ed1940e0268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(1,5,1,3)\n",
    "print(\"A=\\n\",A)\n",
    "B = output_layer(A)\n",
    "print(\"A=\\n\",B)\n",
    "\n",
    "fig,ax = plt.subplots(ncols=2,figsize=(6,3),dpi=200)\n",
    "PlotArray(ax[0],A[0,:,0,:])\n",
    "PlotArray(ax[1],B[0,:,0,:])\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Output\")\n",
    "ax[0].set_ylabel(\"2. axis/dimension\\n (x-axis of the image)\")\n",
    "ax[0].set_xlabel(\"4. axis/dimension\\n (channel axis of the image)\")\n",
    "ax[1].set_ylabel(\"2. axis/dimension\\n (x-axis of the image)\")\n",
    "ax[1].set_xlabel(\"4. axis/dimension\\n (channel axis of the image)\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da26c0f-8733-4703-9912-c119bb647b19",
   "metadata": {},
   "source": [
    "#### 2.2.3 Tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651c99a-5993-4a97-88c3-b7f3fb497304",
   "metadata": {},
   "source": [
    "In TensorFlow, a model is defined as shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556489b-07c1-4165-8726-687cd5dcc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nput_layer = tf.keras.layers.Input(shape=(9, 9, 1))\n",
    "conv_layer = tf.keras.layers.Conv2D(1, kernel_size=(3, 3), padding=\"same\")(input_layer)\n",
    "activation_layer = MishLayer()(conv_layer)\n",
    "output_layer = tf.keras.layers.Conv2D(3, (1,1),padding=\"same\",activation = \"softmax\")(activation_layer)\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e149c-95a7-4bb6-a960-051c33d8ea72",
   "metadata": {},
   "source": [
    "Often in TensorFlow, parameters are referred to as weights, and they are the elements that are trained. These weights of our initialized (untrained) model can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16954ca4-f294-4352-998b-7d1d3f1ec5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316195ce-2a2a-48e8-a375-064900a21afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_data = np.random.randn(1,9,9, 1).astype(np.float32)\n",
    "print(\"input_data=\\n\",input_data)\n",
    "output_data = model(input_data)\n",
    "print(\"output_data=\\n\",output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25712e9f-91bc-4eae-a782-be0df237104c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "890e723f-4cd2-4788-8f94-0cce060632f8",
   "metadata": {},
   "source": [
    "### 2.3. Building the Skyrmion U-Net model/architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78dab505-2c0a-4524-8fb5-c844dbabe854",
   "metadata": {},
   "source": [
    "The U-Net architecture (Ronneberger et al., *LNCS*, Vol. 9351: 234-241 (2015)) generally consists of a:\n",
    "\n",
    "**Contraction path**  \n",
    "- Downsampling; acts as a decoder  \n",
    "- Feature channels double, image size is halved  \n",
    "- Consists of convolution and max pooling layer\n",
    "- Usually applied consecutively\n",
    "\n",
    "**Expansion path**  \n",
    "- Up-sampling; acts as an encoder  \n",
    "- Feature channels reduce, image size increases  \n",
    "- Consists of up-convolution and convolution  \n",
    "- Concatenation with high-resolution features from the left\n",
    "- Usually applied consecutively\n",
    "\n",
    "\n",
    "\n",
    "**Last layer:** 1x1 convolution + softmax  \n",
    "\n",
    "**Advantage of U-Net:** Few data are necessary for good quality  \n",
    "\n",
    "We will now consider a \"Mini\" U-Net architecture. The normal (larger) U-Net architecture is defined further below; everything is the same, only certain parameters are increased. For this larger U-Net, the training would take longer.  \n",
    "\n",
    "The Mini U-Net is defined as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae568dd-5aae-41c6-9564-a8fb80077ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/u_net_architecture_2.png\",300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd77553-713f-4aba-902f-05253aa60fdc",
   "metadata": {},
   "source": [
    "U-Net can be described as follows:\n",
    "\n",
    "$$ \\mathbf{z}_j = \\mathbf{f}_{\\vec{\\theta}}(\\mathbf{x}_j) $$\n",
    "\n",
    "where $\\vec{\\theta}$ is a high-dimensional vector containing all parameters, $\\mathbf{z}_j$ is a 3-dimensional output tensor, and $\\mathbf{x}_j$ is the input Kerr microscopy image ($j$ is a index to label different tensors). In $\\mathbf{f}$ are encapsulated all the operations such as convolution, max pooling, up-convolution, batch normalization, ... .\n",
    "\n",
    "$\\mathbf{z}_j$ is converted to probabilities using softmax:\n",
    "\n",
    "$$ p_{j,x,y,i} = \\frac{\\exp(\\mathbf{z}_{j,x,y,i})}{\\sum_{i\\in \\mathrm{classes}} \\exp(\\mathbf{z}_{j,x,y,i})} $$\n",
    "\n",
    "The output mask $\\mathbf{m}_j$ is then determined by:\n",
    "\n",
    "$$ \\mathbf{m}_{j,x,y} = \\mathrm{arg\\,max}_i\\; p_{j,x,y,i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f84ad-16d3-4a80-a4a1-f18badb17c60",
   "metadata": {},
   "source": [
    "Now we can define the U-Net architecture using these functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e5d13-f818-4d13-ae52-264840d1db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(param):\n",
    "    input = tf.keras.layers.Input(shape=param[\"input_shape\"]+(1,))\n",
    "    next_input = input\n",
    "    \n",
    "    l_residual_con = []\n",
    "    for i in range(param[\"n_depth\"]):\n",
    "        residual_con,next_input = downsample_block(next_input, \n",
    "                                                   (2**i)*param[\"filter_multiplier\"],\n",
    "                                                   param)\n",
    "        l_residual_con.append(residual_con)\n",
    "\n",
    "    next_input = double_conv_block(next_input,\n",
    "                                   (2**param[\"n_depth\"])*param[\"filter_multiplier\"],\n",
    "                                   param)\n",
    "\n",
    "    for i in range(param[\"n_depth\"]):\n",
    "        next_input = upsample_block(next_input,\n",
    "                                    l_residual_con[param[\"n_depth\"]-1-i],\n",
    "                                    (2**(param[\"n_depth\"]-1-i))*param[\"filter_multiplier\"],\n",
    "                                    param)\n",
    "\n",
    "    output = tf.keras.layers.Conv2D(param[\"n_class\"], \n",
    "                                    (1,1),\n",
    "                                    padding=\"same\",\n",
    "                                    activation = \"softmax\",\n",
    "                                    dtype='float32')(next_input)    \n",
    "    \n",
    "    return tf.keras.Model(input, output, name=param[\"name\"])\n",
    "\n",
    "\n",
    "model = get_unet({\"name\":\"unet\",\n",
    "                  \"input_shape\": (512,672), \n",
    "                  \"n_class\": 3,\n",
    "                  \"filter_multiplier\": 5,\n",
    "                  \"n_depth\": 1,\n",
    "                  \"kernel_initialization\": \"he_normal\",\n",
    "                  \"dropout\": 0.01,\n",
    "                  \"kernel_size\": (8,8),\n",
    "                  \"upsample_channel_multiplier\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6808705-dfe2-4457-aaf2-73ca501a77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadececd-fcfb-49d7-a718-66028995cd85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7db7183-65f7-401b-ace1-a26b0f3e1c9f",
   "metadata": {},
   "source": [
    "#### 2.3.1 Large (normal) Skyrmion U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abaad9a-0f7f-48ad-b422-c12a7e696782",
   "metadata": {},
   "source": [
    "The normal, larger U-Net is defined further below in a section. Here is the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69fcb9-2d54-4b3a-9f6d-3781c7f89e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/u_net_architecture_3.png\",320)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde3792-423a-45ca-b715-752446b1eabc",
   "metadata": {},
   "source": [
    "It can also be created using the function `get_unet`, with larger parameters. For the normal (larger) U-Net, we would define:\n",
    "\n",
    "```python\n",
    "model = get_unet({\n",
    "    \"name\": \"unet\",\n",
    "    \"input_shape\": img_size,\n",
    "    \"n_class\": 3,\n",
    "    \"filter_multiplier\": 16,\n",
    "    \"n_depth\": 4,\n",
    "    \"kernel_initialization\": \"he_normal\",\n",
    "    \"dropout\": 0.1,\n",
    "    \"kernel_size\": (3, 3),\n",
    "    \"upsample_channel_multiplier\": 8\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0af701-d40a-4293-914a-a76b15cc1d38",
   "metadata": {},
   "source": [
    "## 3. Loss function\n",
    "### 3.1. Loss function and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a5ac4-6ea4-4693-8a41-ce5683926360",
   "metadata": {},
   "source": [
    "AI models can be described as follows:\n",
    "\n",
    "$$ \\mathbf{z} = \\mathbf{f}_{\\vec{\\theta}}(\\mathbf{x}) $$\n",
    "\n",
    "where $\\vec{\\theta}$ is a high-dimensional vector containing all parameters, $\\mathbf{z}$ is the output tensor, and $\\mathbf{x}$ is the input. Multiple function calls $ \\mathbf{f}_{\\vec{\\theta}}(\\mathbf{x}_i) $ for several data points $\\mathbf{x}_i, i\\in \\{1,\\ldots,n\\}$ are written as $ \\mathbf{f}_{\\vec{\\theta}}(\\{x_i\\}) $ in the following.\n",
    "\n",
    "The loss function $ L(\\mathbf{f}_{\\vec{\\theta}}(\\{x_i\\}), \\{y_i\\}) $ quantifies the error between the model's predictions $ \\mathbf{f}_{\\vec{\\theta}}(\\{x_i\\}) $ and the true labels $ \\{y_i\\} $ corresponding to the data points $ \\{x_i\\} $. It measures how well the model's output matches the true labels, and minimizing this loss is the objective during training. If the error between the model's predictions and the true labels (groundtruth) is large, the loss function will also yield a large value, indicating poor model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb493bd3-8fb9-4723-8652-73c34389bad1",
   "metadata": {},
   "source": [
    "The training of a neural network aims to obtain the global minimum, i.e., it satisfies:\n",
    "\n",
    "$$ \\nabla_{\\vec{\\theta}} L(\\mathbf{f}_{\\vec{\\theta}}(\\{x_i\\}), \\{y_i\\}) = 0 $$\n",
    "\n",
    "To achieve this, the network must undergo training. The simplest approach is:\n",
    "\n",
    "$$ \\vec{\\theta}_{i+1} = \\vec{\\theta}_{i} - \\epsilon \\left. \\nabla_{\\vec{\\theta}_{i}} L(\\mathbf{f}_{\\vec{\\theta}_{i}}(\\{x_i\\}), \\{y_i\\})  \\right|_{\\vec{\\theta}_i}, $$\n",
    "\n",
    "where $\\epsilon$ is the learning rate.\n",
    "\n",
    "However, in this session, for training the large skyrmion U-Net, we employed more advanced and superior training algorithms. The gradient $ \\nabla_{\\vec{\\theta}} L $ can be calculated using backpropagation, which leverages the chain rule for derivatives.\n",
    "\n",
    "The update of $ \\vec{\\theta} $ using the gradient of $ L(\\mathbf{f}_{\\vec{\\theta}}(\\{x_i\\}), \\{y_i\\})  $ is typically done in such a way that $ \\{x_i\\} $ is not the entire dataset but rather a subset called a **batch**. Batches are used in machine learning to fit data into limited RAM and improve training efficiency while stabilizing gradient updates. This is not important for our mini example but becomes crucial with large datasets. When all the data points in the dataset (divided into batches) have been processed once, this is referred to as an **epoch**. Generally, the model is trained over several epochs to achieve optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f41420-c1b7-453f-96fe-0afc66a624c0",
   "metadata": {},
   "source": [
    "### 3.2. Simple Basic Example of Loss Functions\n",
    "\n",
    "Verry simple example that demonstrates training, batches and epochs.\n",
    "\n",
    "We now use this function (with fixed parameters $a,b,c,d$) to generate data points with Gaussian noise and learn from the data points the parameters $a$, $b$, $c$, and $d$:\n",
    "$$f(x) = a \\tanh(c x + d) + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ef7e0-0477-4822-865d-842a85453fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 1000\n",
    "xmin,xmax = -10,10\n",
    "x =  np.linspace(xmin,xmax,n_data)\n",
    "ap,bp,cp,dp = 0.4,1.5,0.6,1.6\n",
    "y = ap*np.tanh(cp*x+dp)+bp\n",
    "x += np.random.randn(n_data)*0.04\n",
    "y += np.random.randn(n_data)*0.04\n",
    "data = np.vstack((x,y)).T\n",
    "fig,ax = plt.subplots(dpi=100)\n",
    "ax.plot(data[:,0],data[:,1],\"ro\",ms=1,label=\"Data function\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "xl = np.linspace(xmin,xmax)\n",
    "ax.plot(xl,ap*np.tanh(cp*xl+dp)+bp,color=\"green\",label=\"Original function\",lw=2,zorder=9000)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d16a7-6544-4838-9c14-25cc868e0180",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The trainings data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2ff2c-4f1a-454f-9e0b-8827bfde543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e2af0-a3c2-4365-ba5a-179787a694c6",
   "metadata": {},
   "source": [
    "Our model, which we aim to learn, is given by:\n",
    "\n",
    "$$f(x)=a \\tanh(c x + d) +b $$\n",
    "\n",
    "where $ \\theta = (a, b, c, d) $. Next, we define a loss function:\n",
    "\n",
    "$$L(\\{Y_i\\},\\{y_i\\}) = \\sum_i (Y_i-y_i)^2$$\n",
    "\n",
    "where the summation runs over all the values in the batch.\n",
    "\n",
    "The parameter update equation,\n",
    "\n",
    "$$ \\vec{\\theta}_{i+1} = \\vec{\\theta}_{i} - \\epsilon \\left. \\nabla_{\\vec{\\theta}} L(\\mathbf{f}_{\\vec{\\theta}}(\\{x_i\\}), \\{y_i\\})  \\right|_{\\vec{\\theta}_i}, $$\n",
    "then becomes with $\\Delta= a_i \\tanh(c x_i + d_i) + b_i-y_i$ :\n",
    "\n",
    "$$ a_{i+1} = a_{i} - \\epsilon \\frac{\\partial}{\\partial a_i}  \\sum_i \\Delta_i^2 = a_{i} - \\epsilon   \\sum_i 2 \\tanh(c x_i + d_i) \\Delta_i$$\n",
    "$$ b_{i+1} = b_{i} - \\epsilon \\frac{\\partial}{\\partial b_i}  \\sum_i \\Delta_i^2 = b_{i} - \\epsilon   \\sum_i 2  \\Delta_i $$\n",
    "$$ c_{i+1} = c_{i} - \\epsilon \\frac{\\partial}{\\partial c_i}  \\sum_i \\Delta_i^2 = a_{i} - \\epsilon \\sum_i 2  a_i x_i (1-\\tanh(c x_i + d_i)^2) \\Delta_i $$\n",
    "$$ d_{i+1} = d_{i} - \\epsilon \\frac{\\partial}{\\partial d_i}  \\sum_i \\Delta_i^2 = d_{i} - \\epsilon   \\sum_i 2 a_i  (1-\\tanh(c x_i + d_i)^2) \\Delta_i $$\n",
    "\n",
    "\n",
    "where is the prediction error and the summation again runs over all the values in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52382b6c-c1c2-49a5-89d1-45b0d00a35b5",
   "metadata": {},
   "source": [
    "Split the data into batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060c1ad-6277-4da5-add3-af38f5966e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_batch = 100\n",
    "#split the data in batches\n",
    "batches = [data[i*n_batch:min((i+1)*n_batch,n_data)] for i in range(int(np.ceil(n_data/n_batch)))]\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd818cf-7e37-4d75-b6c9-63ff4036c382",
   "metadata": {},
   "source": [
    "Now train the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49adb94-1b8d-46e0-b6b9-b88e81e04bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "epsilon = 3e-3 #learning rate\n",
    "a,b,c,d = 1,1,1,1# initial training parameters\n",
    "training_history = []\n",
    "for epoch in range(n_epochs):\n",
    "    loss = 0\n",
    "    for i in range(len(batches)):\n",
    "        x_batch,y_batch = batches[i][:,0],batches[i][:,1]\n",
    "        y_pred = a * np.tanh(c * x_batch + d) + b\n",
    "        loss += np.sum((y_pred - y_batch)**2)\n",
    "    training_history.append((a,b,c,d,loss))\n",
    "\n",
    "    \n",
    "    for i in range(len(batches)):\n",
    "        x_batch,y_batch = batches[i][:,0],batches[i][:,1]\n",
    "        y_pred = a * np.tanh(c * x_batch + d) + b\n",
    "        error = y_pred - y_batch\n",
    "        an = a - epsilon * np.sum(2 * error * np.tanh(c * x_batch + d))\n",
    "        bn = b - epsilon * np.sum(2 * error)\n",
    "        cn = c - epsilon * np.sum(2 * a * x_batch * (1 - np.tanh(c * x_batch + d)**2) * error)\n",
    "        dn = d - epsilon * np.sum(2 * a * (1 - np.tanh(c * x_batch + d)**2) * error)\n",
    "        a,b,c,d = an,bn,cn,dn\n",
    "        \n",
    "    if epoch%100==0:\n",
    "        print(f\"Epoch {epoch}: a={a:.3f},b={b:.3f},c={c:.3f},d={d:.3f}\")\n",
    "    \n",
    "#calculate loss of last epoch\n",
    "loss = 0\n",
    "for i in range(len(batches)):\n",
    "    x_batch,y_batch = batches[i][:,0],batches[i][:,1]\n",
    "    y_pred = a * np.tanh(c * x_batch + d) + b\n",
    "    loss += np.sum((y_pred - y_batch)**2)\n",
    "training_history.append((a,b,c,d,loss))\n",
    "\n",
    "training_history = np.array(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d693e20-4ad0-4613-84b7-6564ce81afe4",
   "metadata": {},
   "source": [
    "Let's take a look at the training history, starting with the evolution of the training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027b23e-7d99-4750-b225-637b84f5bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(dpi=300,figsize=(9,4))\n",
    "ax.set_title(\"Training History\")\n",
    "ax.set_ylabel(r\"Training parameters\")\n",
    "ax.set_xlabel(r\"Epoch\")\n",
    "\n",
    "ax.plot(np.arange(1,1+len(training_history)),training_history[:,0],label=\"a\",color=\"#1f77b4\")\n",
    "ax.axhline(ap,label=\"a (Ground truth)\",color=\"#1f77b4\",ls=\"--\",zorder=-900)\n",
    "ax.plot(np.arange(1,1+len(training_history)),training_history[:,1],label=\"b\",color='#ff7f0e')\n",
    "ax.axhline(bp,label=\"b (Ground truth)\",color=\"#ff7f0e\",ls=\"--\",zorder=-900)\n",
    "ax.plot(np.arange(1,1+len(training_history)),training_history[:,2],label=\"c\",color='#2ca02c')\n",
    "ax.axhline(cp,label=\"c (Ground truth)\",color=\"#2ca02c\",ls=\"--\",zorder=-900)\n",
    "ax.plot(np.arange(1,1+len(training_history)),training_history[:,3],label=\"d\",color='#d62728')\n",
    "ax.axhline(dp,label=\"d (Ground truth)\",color=\"#d62728\",ls=\"--\",zorder=-900)\n",
    "ax.legend(ncols=2)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81bc6a-1e8c-42bc-82fe-0c06cf358856",
   "metadata": {},
   "source": [
    "And now, let's look at the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa74ee-46dd-48b7-997d-a39bbc492dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(dpi=300,figsize=(9,4),ncols=2)\n",
    "ax[0].set_title(\"Training History\")\n",
    "ax[0].set_ylabel(r\"Loss $L$\")\n",
    "ax[0].set_xlabel(r\"Epoch\")\n",
    "ax[0].plot(np.arange(1,1+len((training_history))),training_history[:,4])\n",
    "ax[1].set_title(\"Training History, shifted Loss function\")\n",
    "ax[1].set_ylabel(r\"Loss $L-L_0$\")\n",
    "ax[1].set_xlabel(r\"Epoch\")\n",
    "ax[1].plot(np.arange(1,1+len((training_history))),training_history[:,4]-np.min(training_history[:,4]))\n",
    "ax[1].set_yscale(\"log\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a29221-0b22-4bf6-b9ff-15ec923b0de5",
   "metadata": {},
   "source": [
    "And this is how the final trained (fitted) function looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6596b-b2c0-4eb7-bbee-59cbfcc3a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = np.linspace(xmin,xmax)\n",
    "fig,ax = plt.subplots(dpi=100)\n",
    "ax.plot(data[:,0],data[:,1],\"ro\",ms=1)\n",
    "ax.plot(data[:,0],data[:,1],\"ro\",ms=1,label=\"Data points\")\n",
    "ax.plot(xl,a*np.tanh(c*xl+d)+b,label=\"Trained function\",lw=5)\n",
    "ax.plot(xl,ap*np.tanh(cp*xl+dp)+bp,color=\"green\",label=\"Original function\",lw=2,zorder=9000)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ebe91-2eb8-4e38-834e-308ba4aa1a9a",
   "metadata": {},
   "source": [
    "### 3.3 Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739bc6b0-fa30-4803-99f6-ce1e756038ec",
   "metadata": {},
   "source": [
    "U-Net can be described as follows:\n",
    "\n",
    "$$ \\mathbf{z} = \\mathbf{f}_{\\vec{\\theta}}(\\mathbf{x}) $$\n",
    "\n",
    "where $\\vec{\\theta}$ is a high-dimensional vector containing all parameters, $\\mathbf{z}$ is a 3-dimensional output tensor, and $\\mathbf{x}$ is the input Kerr microscopy image. In $\\mathbf{f}$ are encapsulated all the operations such as convolution, max pooling, up-convolution, batch normalization, ... .\n",
    "\n",
    "$\\mathbf{z}$ is converted to probabilities using softmax:\n",
    "\n",
    "$$ p_{(x,y),i} = \\frac{\\exp(\\mathbf{z}_{(x,y),i})}{\\sum_{i\\in \\mathrm{classes}} \\exp(\\mathbf{z}_{(x,y),i})} $$\n",
    "\n",
    "The output mask $\\mathbf{m}$ is then determined by:\n",
    "\n",
    "$$ \\mathbf{m}_{(x,y)} = \\mathrm{arg\\,max}_i\\; p_{(x,y),i}$$\n",
    "\n",
    "The cross-entropy loss (with averaging over all the pixels) is given by:\n",
    "\n",
    "$$ L = - \\frac{1}{\\sum_{x,y} 1} \\sum_{x,y} \\sum_{i=1}^3 \\;w_i\\;(p_{\\mathrm{ground-truth}})_{(x,y),i} \\log(p_{(x,y),i}) $$\n",
    "\n",
    "where $\\sum_{(x,y)}$ in our case sums over several examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e8fd3-ce20-41c1-b365-8211e8b5b0d6",
   "metadata": {},
   "source": [
    "Let's define the cross-entropy function. This function returns a loss entropy value based on the weights assigned to the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3100e450-ba6d-464f-893b-275f05eed66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(weight):\n",
    "    weight = tf.convert_to_tensor(weight,dtype=np.float32)\n",
    "    def loss(ytrue,ypred):\n",
    "        p = tf.clip_by_value(ypred/(tf.math.reduce_sum(ypred,axis=-1,keepdims=True)),tf.keras.backend.epsilon(),1-tf.keras.backend.epsilon())\n",
    "        return -tf.math.reduce_mean(tf.math.reduce_sum(weight*ytrue*tf.math.log(p),axis=-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0204b-4a90-45b7-bb62-7aac370d4be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cross_entropy_loss([6,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf6041-72c6-4277-9a45-5f8cadb46363",
   "metadata": {},
   "source": [
    "## 4. Dataset processing for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2b81c-e2a5-4273-bc84-615b9f7fbb30",
   "metadata": {},
   "source": [
    "Define a plot function to plot several images in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1443dbd-5b06-47d3-8ee3-64a13eb84dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(l,ltitle=None,nrows=None,ncols=None,dpi=100,s0=1,suptitle=None):\n",
    "    if nrows==None and ncols==None:\n",
    "        nc = int(np.ceil(np.sqrt(len(l))))\n",
    "        nr = int(np.ceil(len(l)/nc))\n",
    "    if ncols!=None and ncols!=None:\n",
    "        nc = ncols\n",
    "        nr = nrows\n",
    "    if nrows!=None and ncols==None:\n",
    "        nr = nrows\n",
    "        nc = int(np.ceil(len(l)/nr))\n",
    "    if ncols!=None and nrows==None:\n",
    "        nc = ncols\n",
    "        nr = int(np.ceil(len(l)/nc))\n",
    "    fig,ax = plt.subplots(nrows=nr,ncols=nc,dpi=dpi,figsize=(nc*s0,nr*s0))\n",
    "    if suptitle != None:\n",
    "        fig.suptitle(suptitle,fontsize=40,y=0.99)\n",
    "    ax = ax.ravel()\n",
    "    for i in range(len(ax)):\n",
    "        ax[i].axis(\"off\")\n",
    "    \n",
    "    for i in range(min(len(l),len(ax))):\n",
    "        ax[i].imshow(l[i],cmap=\"gray\")\n",
    "        if ltitle!= None:\n",
    "            ax[i].set_title(ltitle[i])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecd7b2-3907-48f9-b32d-cb786ff9adfb",
   "metadata": {},
   "source": [
    "### 4.1 Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b585d-2969-4adb-912e-ebdc398adc5c",
   "metadata": {},
   "source": [
    "Augmentation is used in model training to enhance the diversity of the training data, improve model generalization, and reduce overfitting. The augmentation is performed using the Albumentations package, which offers a wide range of powerful and efficient image transformation techniques. These techniques include random flipping, rotation, Gaussian noise, affine transformations (such as translation, scaling, and rotation), and adjustments to brightness and contrast — all of which simulate real-world variations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba0ef8-0226-4de1-849d-2cc2e4813ba1",
   "metadata": {},
   "source": [
    "Define a data function that returns an augmentation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24626678-511a-4bb6-83c4-3ecac44fa5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.GaussNoise(p=1,std_range=(0,25/255)),\n",
    "        albumentations.Affine(translate_percent=0.5,   scale=(0.85, 1.4),  rotate=(-90, 90),p=1,keep_ratio=True,border_mode=cv2.BORDER_REFLECT_101),\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=0.3,contrast_limit=0.5,p=1)], p=1)\n",
    "aug = get_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e027d-0f28-449f-824d-e3d8cf525ef8",
   "metadata": {},
   "source": [
    "Make an examples with this augmentation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8427c-e383-4fac-af85-eefdb7dc739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexample_img = []\n",
    "lexample_aug = []\n",
    "for ix,subtable in dataset_table.groupby(\"source_id\"):\n",
    "    if ix not in [7, 8, 10, 11, 13, 23, 135]: continue\n",
    "    img = (np.array(Image.open(subtable.iloc[0].img_fn)))  \n",
    "    lexample_img.append(img)\n",
    "    lexample_aug.append(aug(image=img)[\"image\"])\n",
    "\n",
    "plot_results(lexample_img,nrows=1,suptitle=\"Original images\",s0=5,dpi=40)\n",
    "plot_results(lexample_aug,nrows=1,suptitle=\"Augmented images\",s0=5,dpi=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63080674-fb0d-4d86-93b8-12473444f2d7",
   "metadata": {},
   "source": [
    "The augmented images may sometimes appear as if they don't originate from the original images. However, this is not the case; they are all results of augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb62d21-d0dd-4a2e-8467-165cfdb5a011",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "For our training, we use the following data augmentation with slightly different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c7f8c-ac7d-4f93-b744-219fb857697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.GaussNoise(p=1,std_range=(0,25/255)),\n",
    "        albumentations.Affine(translate_percent=0.5,   scale=(0.85, 1.4),  rotate=(-90, 90),p=1,keep_ratio=True,border_mode=cv2.BORDER_REFLECT_101),\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=0.25,contrast_limit=0.25,p=1)], p=1)\n",
    "\n",
    "aug = get_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855cc7c-fb45-49b9-a475-d5742473a239",
   "metadata": {},
   "source": [
    "And again some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753aced-d5f6-46a7-ad75-b84c109b87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexample_img = []\n",
    "lexample_aug = []\n",
    "for ix,subtable in dataset_table.groupby(\"source_id\"):\n",
    "    if ix not in [7, 8, 10, 11, 13, 23, 135]: continue\n",
    "    img = (np.array(Image.open(subtable.iloc[0].img_fn)))  \n",
    "    lexample_img.append(img)\n",
    "    lexample_aug.append(aug(image=img)[\"image\"])\n",
    "\n",
    "plot_results(lexample_img,nrows=1,suptitle=\"Original images\",s0=5,dpi=40)\n",
    "plot_results(lexample_aug,nrows=1,suptitle=\"Augmented images\",s0=5,dpi=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64957f29-9b65-4bac-ae90-21cec9a38995",
   "metadata": {},
   "source": [
    "### 4.2 Label smoothing\n",
    "\n",
    "We apply label smoothing to the output mask to enhance training performance. This technique blends the original labels with a uniform distribution, preventing the model from relying too heavily on exact label values and enhancing generalization. The smoothed probability is defined as follows: \n",
    "$$p_{(x,y),i,\\mathrm{training}} = (1-f) p_{(x,y),i,\\mathrm{label}} + \\frac{f}{\\# \\text{ classes}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2f02e-3cce-4c3e-afbf-b50ab5c5976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothing(x,f,num_classes):\n",
    "    return (1-f)*x+f/num_classes\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3,dpi=300,figsize=(12,2.9),constrained_layout=True)\n",
    "im = ax[0].imshow(segmentation_mask_label_class,cmap=\"gray\",vmin=0,vmax=2)\n",
    "col = fig.colorbar(im,ax=ax[0])\n",
    "ax[0].set_title(\"Segmentation mask\"+\"\\n\"+r\"No label smoothing $f=0$\")\n",
    "\n",
    "im = ax[1].imshow(label_smoothing(segmentation_mask_label_class,0.1,3),cmap=\"gray\",vmin=0,vmax=2)\n",
    "col = fig.colorbar(im,ax=ax[1])\n",
    "ax[1].set_title(\"Segmentation mask\"+\"\\n\"+r\"Label smoothing $f=0.1$\")\n",
    "\n",
    "im = ax[2].imshow(label_smoothing(segmentation_mask_label_class,0.9,3),cmap=\"gray\",vmin=0,vmax=2)\n",
    "col = fig.colorbar(im,ax=ax[2])\n",
    "ax[2].set_title(\"Segmentation mask\"+\"\\n\"+r\"Label smoothing $f=0.9$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d3424-2cf1-4960-ad99-cdbf86055038",
   "metadata": {},
   "source": [
    "### 4.3 Data generator\n",
    "\n",
    "A data generator efficiently loads and feeds data in batches during training, ideal for large datasets. It supports real-time augmentation, image shuffling, and label smoothing.\n",
    "\n",
    "The data generator also renormalizes the frames from RGB 8-bit values (0-255) to floating-point values in the range 0-1, as data normalization is always required for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958381d-8c5c-4203-bb75-264730c0fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Training'\n",
    "    def __init__(self, images, labels, batch_size, n_class=3, smoothing=False,shuffle=True, aug=None):\n",
    "        super().__init__()\n",
    "        'Initialization'\n",
    "        self.n_class = n_class\n",
    "        if labels.shape[:3] != images.shape[:3]:\n",
    "            raise Exception(\"Shape not fit\")\n",
    "\n",
    "        self.len_data = labels.shape[0]\n",
    "        self.shape_data = labels.shape[1:3]\n",
    "        self.labels = labels\n",
    "        self.images = images\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "        self.aug = aug\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len_data)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.len_data/ self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X,y = (self.images[indexes]).copy(), (self.labels[indexes]).copy()    \n",
    "        if self.aug is not None:      \n",
    "            self.data_augmentation(X, y)\n",
    "        y = y.astype(dtype=np.float16)\n",
    "        if self.smoothing:\n",
    "            y[y==0] = self.smooth_labels(0.0,factor=0.2)\n",
    "            y[y==1] = self.smooth_labels(1.0,factor=0.2)\n",
    "        return X/255,y\n",
    "        \n",
    "    def smooth_labels(self, labels, factor=0.1):\n",
    "        return labels*(1 - factor)+(factor / self.n_class)\n",
    "    \n",
    "    def data_augmentation(self, X, y):\n",
    "        for i in range(self.batch_size):\n",
    "            augmented = self.aug(image=X[i],mask=y[i])\n",
    "            X[i] = augmented[\"image\"]\n",
    "            y[i] = augmented[\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331f988-b99d-471a-97eb-8671ab6bdb5b",
   "metadata": {},
   "source": [
    "### 4.4 Splitting of Dataset into Training, Validation, and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2a731-27ad-47e1-a32e-8f76a43c2c6f",
   "metadata": {},
   "source": [
    "For training, we split our data set into training, validation, and test sets. The training set is used to train the model by allowing it to learn the underlying patterns in the data. The validation set is used to tune hyperparameters and assess the model’s performance during training to prevent overfitting. Finally, the test set is employed to evaluate the model’s generalization ability on unseen data, providing a realistic measure of its performance in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fee63c-8c5a-450b-92f5-fb82fd6bdf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8, 1),dpi=200)\n",
    "ax.text(0.2, 0.7, 'Training Set', ha='center', va='center', fontsize=12, bbox=dict(facecolor='#4CAF50',lw=0, alpha=0.6,pad=5))\n",
    "ax.text(0.5, 0.7, 'Validation Set', ha='center', va='center', fontsize=12, bbox=dict(facecolor='#2196F3',lw=0, alpha=0.6,pad=5))\n",
    "ax.text(0.85, 0.7, 'Test Set', ha='center', va='center', fontsize=12, bbox=dict(facecolor='#FF9800',lw=0, alpha=0.6,pad=5))\n",
    "ax.text(0.35, 0.9,'Used during training process', ha='center')\n",
    "ax.text(0.86, 0.9,'Used for model evaluation',ha='center')\n",
    "ax.axis('off')\n",
    "ax.set_title('Data Split for AI Model Training')\n",
    "ax.set_ylim(0.6,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8573df-0899-44ed-bcaa-847a46f88a5a",
   "metadata": {},
   "source": [
    "The same source ID indicates that the images/frames originate from the same video. Therefore, we split the train, test, and validation sets such that images/frames from a single video remain in the same set. Thus, the training, validation, and test sets are defined based on the source ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135988d-1143-4965-8670-50cdef0f898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ix = np.array([ix for ele in sorted(list(set([10,11,20,23,7,135]))) for ix in dataset_table[dataset_table.source_id==ele].index.to_numpy()])\n",
    "test_ix = np.array([ix for ele in sorted(list(set([6,9]))) for ix in dataset_table[dataset_table.source_id==ele].index.to_numpy()])\n",
    "val_ix = np.array([ix for ele in sorted(list(set([8,13]))) for ix in dataset_table[dataset_table.source_id==ele].index.to_numpy()])\n",
    "\n",
    "print(train_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30c909-5db2-427c-ae23-db3dff2e9ae1",
   "metadata": {},
   "source": [
    "Only the indices are stored from the data table, so you can then find the image from the data table. Later, all images will also be loaded in the same order as in the data table, and then you can access the images using the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99de8b-cccc-43ac-ac3a-1c369bd34e51",
   "metadata": {},
   "source": [
    "Now an example of one image taken from each video used, along with the split into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aecd86-7940-4ade-a34d-d62f7c821460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_after_source(lix,ncolmax=6,suptitle=None):\n",
    "    table = dataset_table.iloc[lix]\n",
    "    plot_results([np.array(Image.open(b.iloc[0].img_fn)) for a,b in table.groupby(\"source_id\")],[str(a) for a,b in table.groupby(\"source_id\")],suptitle=suptitle,ncols=6,dpi=100,s0=5)\n",
    "group_after_source(train_ix,suptitle=\"Training set\")\n",
    "group_after_source(val_ix,suptitle=\"Validation set\")\n",
    "group_after_source(test_ix,suptitle=\"Test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fd525-5f65-4cbc-9968-31fd86959930",
   "metadata": {},
   "source": [
    "### 4.5 Loading images and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682a0ff-b5c2-4ec3-9051-247ff34a99d7",
   "metadata": {},
   "source": [
    "Loading of images and labels. The loaded arrays/tensors from the function `load_img_label_data` have a shape of \n",
    "$$(\\text{number of images}, \\text{image height}, \\text{image width}).$$\n",
    "\n",
    "The index of the first dimension of these arrays corresponds to the index of the training, test, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ddbdc-d1b5-4910-9b62-b18727a86a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_label_data(lix):\n",
    "    return np.array([Image.open(ele) for ele in dataset_table.iloc[lix].img_fn]), \\\n",
    "           np.array([trafo_rgb_to_class(np.array(Image.open(ele))) for ele in dataset_table.iloc[lix].label_fn])\n",
    "    \n",
    "\n",
    "train_img,train_label = load_img_label_data(train_ix)\n",
    "val_img,val_label = load_img_label_data(val_ix)\n",
    "test_img,test_label = load_img_label_data(test_ix)\n",
    "img_size = test_img.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab71a85-cd69-405f-9532-0e5e7964a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_img.shape=\",train_img.shape)\n",
    "print(\"train_img=\\n\",train_img)\n",
    "\n",
    "print(\"train_label.shape=\",train_label.shape)\n",
    "print(\"train_label=\\n\",train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a53b78-5355-42f8-8c27-c18ba6ae803b",
   "metadata": {},
   "source": [
    "## 5 Benchmarking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91719aa-d348-4e34-a11b-33a4a4169e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_example = train_label[1]\n",
    "fig,ax = plt.subplots(dpi=150,figsize=(6,3))\n",
    "ax.imshow(trafo_class_to_rgb(label_example))\n",
    "ax.set_title(\"Groundtruth\")\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3],3), \n",
    "                    cmap=matplotlib.colors.ListedColormap([(0,0,1),(0,1,0),(1,0,0)])),\n",
    "                    ax=ax, ticks=[0.5, 1.5, 2.5])\n",
    "cbar.set_ticklabels(['Background', 'Defects', 'Skyrmions'])\n",
    "fig.tight_layout()\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3,dpi=300,figsize=(9,3))\n",
    "ax[0].set_title(\"Perfect prediction\")\n",
    "ax[0].imshow(trafo_class_to_rgb(label_example))\n",
    "ax[1].set_title(\"Wrong prediction\\n wrong_label_1\")\n",
    "wrong_label_1 = np.random.randint(0,3,label_example.shape)\n",
    "ax[1].imshow(trafo_class_to_rgb(wrong_label_1))\n",
    "ax[2].set_title(\"Wrong prediction\\n wrong_label_2\")\n",
    "wrong_label_2 = label_example.copy()\n",
    "wrong_label_2[wrong_label_2==0] = 5\n",
    "wrong_label_2[wrong_label_2==2] = 0\n",
    "wrong_label_2[wrong_label_2==5] = 2\n",
    "ax[2].imshow(trafo_class_to_rgb(wrong_label_2))\n",
    "\n",
    "for i in range(3):\n",
    "    cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3],3), \n",
    "                    cmap=matplotlib.colors.ListedColormap([(0,0,1),(0,1,0),(1,0,0)])),\n",
    "                    ax=ax[i], ticks=[0.5, 1.5, 2.5],orientation='horizontal')\n",
    "    cbar.set_ticklabels(['Background', 'Defects', 'Skyrmions'])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89eb400-98ed-405b-b651-af3911c0fbc6",
   "metadata": {},
   "source": [
    "To provide a quantitative description of the benchmarking process, we perform a pixel-wise evaluation and reduce it to a binary classification: \n",
    "- True class (true = skyrmion)\n",
    "- False class (false = defect or background).\n",
    "\n",
    "Using this classification, we can construct the confusion matrix.\n",
    "\n",
    "|                | Actual Positive | Actual Negative |\n",
    "|----------------|-----------------|------------------|\n",
    "| **Predicted Positive** | **TP** (True Positive) | **FP** (False Positive) |\n",
    "| **Predicted Negative** | **FN** (False Negative) | **TN** (True Negative) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe324e75-d067-4ddd-895c-b35815ca0d80",
   "metadata": {},
   "source": [
    "With this, we can define the **Matthews Correlation Coefficient (MCC)**:\n",
    "\n",
    "$$ \\mathrm{MCC} = \\frac{\\mathrm{TP} \\cdot \\mathrm{TN} - \\mathrm{FP} \\cdot \\mathrm{FN}}{\\sqrt{(\\mathrm{TP} + \\mathrm{FP})(\\mathrm{FN} + \\mathrm{TN})(\\mathrm{TP} + \\mathrm{FN})(\\mathrm{FP} + \\mathrm{TN})}} $$\n",
    "\n",
    "where:\n",
    "- $\\mathrm{TP}$ = number of true positives  \n",
    "- $\\mathrm{TN}$ = number of true negatives  \n",
    "- $\\mathrm{FP}$ = number of false positives  \n",
    "- $\\mathrm{FN}$ = number of false negatives  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621f475-fca8-4102-98b7-30a431ddbe33",
   "metadata": {},
   "source": [
    "So let's implement the MCC in code now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b3ba5-d885-4111-bd14-4f3ade67a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TF_PN(y_true,y_pred,ix0):\n",
    "    m1,m2 = y_true==ix0,y_pred==ix0\n",
    "    im1,im2 = tf.math.logical_not(m1),tf.math.logical_not(m2)\n",
    "    TP = tf.math.reduce_mean(tf.cast(tf.math.logical_and(m1,m2),dtype=np.float64))\n",
    "    TN = tf.math.reduce_mean(tf.cast(tf.math.logical_and(im1,im2),dtype=np.float64))\n",
    "    FP = tf.math.reduce_mean(tf.cast(tf.math.logical_and(im1,m2),dtype=np.float64))\n",
    "    FN = tf.math.reduce_mean(tf.cast(tf.math.logical_and(m1,im2),dtype=np.float64))\n",
    "    return TP,TN,FP,FN\n",
    "\n",
    "def get_mcc_from_TF_PN(TP,TN,FP,FN):\n",
    "    denom = tf.keras.ops.sqrt((TP + FN) * (FP + TN) * (FP + TP) * (FN + TN))\n",
    "    val = (TP * TN - FP * FN) / denom\n",
    "    return  tf.where(tf.equal(denom, 0), tf.constant(0, dtype=tf.float64), val)\n",
    "\n",
    "get_mcc = lambda x,y,ix0: get_mcc_from_TF_PN(*get_TF_PN(x,y,ix0)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4ca74-eaf7-4715-9817-b0b55a1de114",
   "metadata": {},
   "source": [
    "For the example above, the MCC is now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571a7d2-77fd-4bb6-bd89-56d24e8cfeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,dpi=300,figsize=(9,3))\n",
    "\n",
    "mcc_1 = get_mcc(label_example,label_example,0)\n",
    "mcc_2 = get_mcc(label_example,wrong_label_1,0)\n",
    "mcc_3 = get_mcc(label_example,wrong_label_2,0)\n",
    "\n",
    "print(\"MCC label_example:\",mcc_1)\n",
    "print(\"MCC wrong_label_1:\",mcc_2)\n",
    "print(\"MCC wrong_label_2:\",mcc_3)\n",
    "\n",
    "ax[0].set_title(f\"MCC={mcc_1:.3f}\"+\"\\n\"+\"Perfect prediction\\n\")\n",
    "ax[0].imshow(trafo_class_to_rgb(label_example))\n",
    "ax[1].set_title(f\"MCC={mcc_2:.3f}\"+\"\\n\"+\"Wrong prediction\\n wrong_label_1\")\n",
    "ax[1].imshow(trafo_class_to_rgb(wrong_label_1))\n",
    "ax[2].set_title(f\"MCC={mcc_3:.3f}\"+\"\\n\"+\"Wrong prediction\\n wrong_label_2\")\n",
    "ax[2].imshow(trafo_class_to_rgb(wrong_label_2))\n",
    "\n",
    "for i in range(3):\n",
    "    cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3],3), \n",
    "                    cmap=matplotlib.colors.ListedColormap([(0,0,1),(0,1,0),(1,0,0)])),\n",
    "                    ax=ax[i], ticks=[0.5, 1.5, 2.5],orientation='horizontal')\n",
    "    cbar.set_ticklabels(['Background', 'Defects', 'Skyrmions'])\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779acca-6e4e-4c87-8a2c-4425962c69fa",
   "metadata": {},
   "source": [
    "Additionally, we need the following code for the MCC calculation during the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e38b50-04a7-4072-bc13-a2a2683aeeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCC(tf.keras.metrics.Metric):\n",
    "    def __init__(self, ix0, name=\"MCC\", **kwargs):\n",
    "        super().__init__(name=name,dtype=tf.float64,**kwargs)\n",
    "        self.ix0 = ix0\n",
    "        self.tp = self.add_variable(shape=(),name=\"TP\", initializer=\"zeros\",dtype=tf.float64)\n",
    "        self.tn = self.add_variable(shape=(),name=\"TN\", initializer=\"zeros\",dtype=tf.float64)\n",
    "        self.fp = self.add_variable(shape=(),name=\"FP\", initializer=\"zeros\",dtype=tf.float64)\n",
    "        self.fn = self.add_variable(shape=(),name=\"FN\", initializer=\"zeros\",dtype=tf.float64)\n",
    "        self.tot = self.add_variable(shape=(),name=\"TOT\", initializer=\"zeros\",dtype=tf.int64)\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        TP,TN,FP,FN = get_TF_PN(tf.argmax(y_true,axis=-1),tf.argmax(y_pred,axis=-1),self.ix0)\n",
    "        totn1 = tf.cast(self.tot+1,tf.float64)\n",
    "        self.tp.assign_add((TP-self.tp)/totn1)\n",
    "        self.tn.assign_add((TN-self.tn)/totn1)\n",
    "        self.fp.assign_add((FP-self.fp)/totn1)\n",
    "        self.fn.assign_add((FN-self.fn)/totn1)\n",
    "        self.tot.assign_add(1)\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.tp.assign(0)\n",
    "        self.tn.assign(0)\n",
    "        self.fp.assign(0)\n",
    "        self.fn.assign(0)\n",
    "        self.tot.assign(0)    \n",
    "    \n",
    "    def result(self):\n",
    "        return get_mcc_from_TF_PN(self.tp,self.tn,self.fp,self.fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ac589-8656-49ee-9513-44c71ecbfc5b",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f11ab0-c6c0-4cf6-ba2f-48fa0866182c",
   "metadata": {},
   "source": [
    "### 6.1 Initialization and Performing the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a32c99-a1c9-4933-a754-0b2649e94db0",
   "metadata": {},
   "source": [
    "Define where the model is stored, initialize the training process, especially the models and the data generator.\n",
    "\n",
    "For training, we use the **Adam optimizer** for training with cross-entropy loss. Additionally, we monitor the training process using the Matthews correlation coefficient (MCC). Furthermore, we define the data generators for the training and validation sets, and we set the batch size to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf335a-8ced-4ad2-a5c6-d5b99fcb7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeln = \"model_small\"\n",
    "folder = basis_dir+\"training/\"\n",
    "modelfn = folder+modeln+\".keras\"\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "model = get_unet({\"name\":\"unet\",\"input_shape\": (512,672), \"n_class\":3,\"filter_multiplier\":5,\"n_depth\":1,\n",
    "                  \"kernel_initialization\":\"he_normal\",\"dropout\":0.01,\"kernel_size\":(8,8),\"upsample_channel_multiplier\":2})\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),loss=get_cross_entropy_loss([6,1,1]),metrics=[\"accuracy\",MCC(0)])\n",
    "\n",
    "batch_size = 5\n",
    "basis = np.eye(3,dtype=np.uint8)\n",
    "dg_train = DataGenerator(train_img,basis[train_label],batch_size,smoothing=False,aug=get_augmentation())\n",
    "dg_val = DataGenerator(val_img,basis[val_label],batch_size,smoothing=False,aug=get_augmentation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9643c-9cfa-4132-ab36-35a4fe992a14",
   "metadata": {},
   "source": [
    "Now we perform the training. We train for 15 epochs, passing the data and the validation training set. Furthermore, we reduce the learning rate if the validation loss does not decrease after a certain number of epochs. Each time the validation loss reaches a new minimum, we save the model. Finally, we save the training history.\n",
    "\n",
    "If the training does not work, a pre-trained small model along with its training history is already saved, which you can load afterwards in case the training doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e502439-1e23-4aa9-bb59-2d5d875cd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=dg_train,validation_data=dg_val,epochs=15,verbose=1,batch_size=batch_size,callbacks=[\n",
    "tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',mode='min',factor=0.5, patience=2, min_lr=1e-6, verbose=1),\n",
    "tf.keras.callbacks.ModelCheckpoint(modelfn, verbose=1,monitor='val_loss',mode='min',save_best_only=True,save_weights_only=False)])\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)  \n",
    "hist_df.to_csv(folder+modeln+\"_history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e2dca-995f-47ea-aec2-cc2bbbf0bc3d",
   "metadata": {},
   "source": [
    "In case the **training fails**, then **execute this cell** to load a training curve and a trained U-Net (with the same architecture as discussed above) from a previous training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb417e-540b-4c6d-9c01-f4f753d60b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    hist_df\n",
    "except:\n",
    "    modeln = \"model_small\"\n",
    "    folder = basis_dir+\"training_result/\"\n",
    "    modelfn = folder+modeln+\".keras\"\n",
    "    hist_df = pd.read_csv(folder+modeln+\"_history.csv\")\n",
    "    mini_unet_loaded_from_repository = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954f0bb-93d6-4038-9213-a1e8e49d13f8",
   "metadata": {},
   "source": [
    "### 6.2 Training history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d4d72-1f73-4b98-a1a1-17bdab90030d",
   "metadata": {},
   "source": [
    "Plot the Training Performance History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb78668e-e32b-497f-8e02-dace9dfced45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,figsize=(8,3),dpi=300)\n",
    "fig.suptitle(\"Training history - Validation Set\",fontsize=14)\n",
    "ax[0].plot(np.arange(1,len(hist_df)+1),hist_df[\"val_loss\"],color=\"indianred\",lw=4)\n",
    "ax[1].plot(np.arange(1,len(hist_df)+1),hist_df[\"val_accuracy\"],color=\"indianred\",lw=4)\n",
    "ax[2].plot(np.arange(1,len(hist_df)+1),hist_df[\"val_MCC\"],color=\"indianred\",lw=4)\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel(\"Epoch\")\n",
    "    ax[i].set_xticks(np.arange(0,len(hist_df)+1,5))\n",
    "    \n",
    "ax[0].set_ylabel(\"Cross-entropy loss\")\n",
    "ax[0].set_title(\"Cross-entropy loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[2].set_ylabel(\"MCC\")\n",
    "ax[2].set_title(\"MCC\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3,figsize=(8,3),dpi=300)\n",
    "fig.suptitle(\"Training history - Trainings Set\",fontsize=14)\n",
    "ax[0].plot(np.arange(1,len(hist_df)+1),hist_df[\"loss\"],color=\"green\",lw=4)\n",
    "ax[1].plot(np.arange(1,len(hist_df)+1),hist_df[\"accuracy\"],color=\"green\",lw=4)\n",
    "ax[2].plot(np.arange(1,len(hist_df)+1),hist_df[\"MCC\"],color=\"green\",lw=4)\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel(\"Epoch\")\n",
    "    ax[i].set_xticks(np.arange(0,len(hist_df)+1,5))\n",
    "ax[0].set_ylabel(\"Cross-entropy loss\")\n",
    "ax[0].set_title(\"Cross-entropy loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[2].set_ylabel(\"MCC\")\n",
    "ax[2].set_title(\"MCC\")\n",
    "fig.tight_layout()\n",
    "\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99c081-fb1c-458d-8c9f-260e99153b66",
   "metadata": {},
   "source": [
    "## 7. Prediction & Benchmarking with the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ceb70-3b91-4c18-a7da-0e8286aa0855",
   "metadata": {},
   "source": [
    "### 7.1 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610bd3d-a332-4bdc-a143-2b44313d7740",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Let's make a prediction on an image of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3c5e2-8a0e-47f3-8313-2f1c854442df",
   "metadata": {},
   "source": [
    "Pick one image out of the test set and normalize the values to the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9649c-3d68-41fc-898b-e020b5a73e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_be_predicted = test_img[0]/255\n",
    "img_to_be_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ff260-d384-410e-a202-cb6339b4482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.imshow(img_to_be_predicted,cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39144329-20ab-4d7c-b55f-aa5de9dd0afd",
   "metadata": {},
   "source": [
    "The filename of the trained model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99f327-62f3-4947-ac64-f494a2a96eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe21014-3f8e-4323-a777-cf972061feda",
   "metadata": {},
   "source": [
    "Load the trained model:\n",
    "\n",
    "(If the training fails and the pre-trained model Mini Skyrmion U-Net is loaded from the repository and executed on the CPU, the model must be redefined because the pre-trained Skyrmion U-Nets were trained on the GPU with float_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa9dfd-afd8-4b79-8476-5aaf07c6fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(modelfn,compile=False,custom_objects={'MishLayer': MishLayer})\n",
    "\n",
    "if (not gpu_available()) and mini_unet_loaded_from_repository:\n",
    "    #create identical model, only with pure float_32 policy\n",
    "    nmodel = get_unet({\"name\":\"unet\",\"input_shape\": (512,672), \"n_class\": 3,\n",
    "              \"filter_multiplier\": 5,\"n_depth\": 1,\"kernel_initialization\": \"he_normal\",\n",
    "              \"dropout\": 0.01,\"kernel_size\": (8,8),\"upsample_channel_multiplier\": 2})\n",
    "    nmodel.set_weights(model.weights)\n",
    "    model = nmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285567a-f571-4708-9061-e6eb0e1289a6",
   "metadata": {},
   "source": [
    "Now make the Prediction with the U-Net. An additional dimension is added around the data since the U-Net input expects batches of images. Since the final layer outputs a probability for each class $i$, an argmax operation must be performed afterward: $ \\mathbf{m}_{(x,y)} = \\mathrm{arg\\,max}_i\\; p_{(x,y),i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d0935-9877-48f2-bdee-c8dc911c5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(np.array([img_to_be_predicted]))[0]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba0cf4-41d6-4d38-82bf-25f3cceaec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_mask = np.argmax(p,axis=-1)\n",
    "segmented_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210fb06-6c1d-4260-964b-95fc448251b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=2,nrows=2,dpi=300,figsize=(6,6),constrained_layout=True)\n",
    "ax = ax.flatten()\n",
    "ax[0].imshow(img_to_be_predicted,cmap=\"gray\")\n",
    "im = ax[1].imshow(segmented_mask,cmap=\"gray\")\n",
    "col = fig.colorbar(im,ax=ax[1])\n",
    "col.set_ticks([0,1,2])\n",
    "ax[2].imshow(trafo_class_to_rgb(segmented_mask))\n",
    "ax[3].imshow(trafo_class_to_rgb(test_label[0]))\n",
    "\n",
    "ax[0].set_title(\"Kerr micrograph\")\n",
    "ax[2].set_title(\"Predicted Mask\\n(RGB code for the 3 classes)\")\n",
    "ax[1].set_title(\"Predicted Mask\\n(Class index for the 3 classes)\")\n",
    "ax[3].set_title(\"Groundtruth Mask\\n(RGB code for the 3 classes)\")\n",
    "\n",
    "for i in [2,3]:\n",
    "    cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3],3), \n",
    "                    cmap=matplotlib.colors.ListedColormap([(0,0,1),(0,1,0),(1,0,0)])),\n",
    "                    ax=ax[i], ticks=[0.5, 1.5, 2.5],orientation='horizontal')\n",
    "    cbar.set_ticklabels(['Background', 'Defects', 'Skyrmions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f1a95-16d7-4afc-92f6-6ed06af1e42e",
   "metadata": {},
   "source": [
    "Now a prediction function that also predicts in batches (with the batches processed in **parallel on GPU**), allowing a large number of images to be predicted efficiently. The principle remains the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4321a1-881f-4940-b9e3-74c0dd22b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,modelfn,batch_size=5,normalize_255=True):\n",
    "    model = tf.keras.models.load_model(modelfn,compile=False,custom_objects={'MishLayer': MishLayer})\n",
    "\n",
    "    if not gpu_available():\n",
    "        #create identical model, only with pure float_32 policy\n",
    "        batch_size = 1\n",
    "        nmodel = get_unet({\"name\":\"unet\",\"input_shape\": (512,672), \"n_class\": 3,\n",
    "                  \"filter_multiplier\": 5,\"n_depth\": 1,\"kernel_initialization\": \"he_normal\",\n",
    "                  \"dropout\": 0.01,\"kernel_size\": (8,8),\"upsample_channel_multiplier\": 2})\n",
    "        nmodel.set_weights(model.weights)\n",
    "        model = nmodel\n",
    "    \n",
    "    n = int(np.ceil(len(x)/batch_size))\n",
    "    lix = [np.array(range(j*batch_size,min((j+1)*batch_size,len(x)))) for j in range(n)]\n",
    "    ylabel = np.zeros(x.shape,dtype=np.uint8)\n",
    "    progbar = tf.keras.utils.Progbar(n)\n",
    "    for i in range(n):            \n",
    "        progbar.update(i)\n",
    "        input = x[lix[i]]\n",
    "        if normalize_255:\n",
    "            input = input/255\n",
    "        ylabel[lix[i]] = model.predict(input,verbose=False).argmax(-1)\n",
    "    progbar.update(n,finalize=True)\n",
    "    return ylabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bc505-dbde-41b2-9b4a-ee979c6ea9e5",
   "metadata": {},
   "source": [
    "And now, prediction of images in the training, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1939805-65a1-4bd2-9abd-60fb87843656",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ix = np.hstack((test_ix,val_ix,train_ix))\n",
    "total_img = np.vstack((test_img,val_img,train_img))\n",
    "total_label = np.vstack((test_label,val_label,train_label))\n",
    "\n",
    "S = set(np.hstack((test_ix,val_ix,train_ix)))&set([b.iloc[0].name for a,b in dataset_table.groupby(\"source_id\")])\n",
    "total_pred = predict(total_img,modelfn,5)\n",
    "for ix0 in range(len(total_img)):\n",
    "    if total_ix[ix0] not in S: continue\n",
    "    fig,ax = plt.subplots(ncols=3,figsize=(8,3),dpi=200)\n",
    "    for e in range(3):\n",
    "        ax[e].axis(\"off\")\n",
    "    ax[0].imshow(total_img[ix0],cmap=\"gray\")\n",
    "    ax[1].imshow(trafo_class_to_rgb(total_label[ix0]))\n",
    "    ax[2].imshow(trafo_class_to_rgb(total_pred[ix0]))\n",
    "    ax[0].set_title(\"Kerr image\")\n",
    "    ax[1].set_title(\"Ground truth\")\n",
    "    ax[2].set_title(\"Predicted label\")\n",
    "    if total_ix[ix0] in train_ix:\n",
    "        fig.suptitle(\"Image used in training\",y=1.02,fontsize=16)\n",
    "    elif total_ix[ix0] in test_ix:\n",
    "        fig.suptitle(\"Image used in testing\",y=1.02,fontsize=16)\n",
    "    elif total_ix[ix0] in val_ix:\n",
    "        fig.suptitle(\"Image used in validation\",y=1.02,fontsize=16)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for i in range(3):\n",
    "        cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3],3), \n",
    "                        cmap=matplotlib.colors.ListedColormap([(0,0,1),(0,1,0),(1,0,0)])),\n",
    "                        ax=ax[i], ticks=[0.5, 1.5, 2.5],orientation='horizontal')\n",
    "        cbar.set_ticklabels(['Background', 'Defects', 'Skyrmions'])\n",
    "        if i == 0:\n",
    "            cbar.ax.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76814a1c-4adf-42db-bc6a-255ac4a4f4db",
   "metadata": {},
   "source": [
    "### 7.2 Benchmarking with MCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e7bcd-649a-42ed-a9a6-0157b5630445",
   "metadata": {},
   "source": [
    "Benchmark with the MCC now the complete dataset by performing a benchmark on all images (validation, test, and train set) and once on the images of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d703e-baeb-454b-8bbb-6f1a84758b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mcc(x,label,modelfn,ix0=0,batch_size=5,normalize_255=True):\n",
    "    model = tf.keras.models.load_model(modelfn,compile=False,custom_objects={'MishLayer': MishLayer})\n",
    "    n = int(np.ceil(len(x)/batch_size))\n",
    "    lix = [np.array(range(j*batch_size,min((j+1)*batch_size,len(x)))) for j in range(n)]\n",
    "    L = []\n",
    "    progbar = tf.keras.utils.Progbar(n)\n",
    "    for i in range(n):            \n",
    "        progbar.update(i)\n",
    "        input = x[lix[i]]\n",
    "        if normalize_255:\n",
    "            input = input/255\n",
    "        output = model.predict(input,verbose=False).argmax(-1)\n",
    "        TP,TN,FP,FN = get_TF_PN(label[lix[i]],output,ix0)\n",
    "        L.append({\"n_size\":tf.reduce_prod(tf.cast(output.shape,dtype=tf.int64)),\"TP\":TP,\"TN\":TN,\"FP\":FP,\"FN\":FN})\n",
    "    progbar.update(n,finalize=True)\n",
    "    tot_n = sum([ele[\"n_size\"] for ele in L])\n",
    "    TP,TN,FP,FN = tf.Variable(0.0,dtype=tf.float64),tf.Variable(0.0,dtype=tf.float64),tf.Variable(0.0,dtype=tf.float64),tf.Variable(0.0,dtype=tf.float64)\n",
    "    for ele in L:\n",
    "        w = ele[\"n_size\"]/tot_n\n",
    "        TP.assign_add(w*ele[\"TP\"])\n",
    "        TN.assign_add(w*ele[\"TN\"])\n",
    "        FP.assign_add(w*ele[\"FP\"])\n",
    "        FN.assign_add(w*ele[\"FN\"])\n",
    "    return get_mcc_from_TF_PN(TP,TN,FP,FN).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7b597-e0cf-4526-8e52-f89c3c08fbaa",
   "metadata": {},
   "source": [
    "Now calculate the values on complete set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35178f-d8e3-405e-9a2f-f98b68c535cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pixelwise Matthews correlation coefficient on test set (true=skyrmion,false={{defect,background}}) \\n= {batch_mcc(val_img,val_label,modelfn,0,batch_size):.3f}\")\n",
    "print(f\"Pixelwise Matthews correlation coefficient  on complete set (true=skyrmion,false={{defect,background}}) \\n= {batch_mcc(total_img,total_label,modelfn,0,batch_size):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be36ba-17ee-4a24-ac49-4e8c866afe5b",
   "metadata": {},
   "source": [
    "## 8. Training a large Skyrmion U-Net training tutorial with large dataset\n",
    "\n",
    "This section demonstrates how to train a large U-Net model using a large dataset. **You do not need to train the following remaining cells in the notebook if you do not want to train the large U-net.**\n",
    "\n",
    "A powerful GPU is required to train the large model; however, the code and concept are the same as what we have done up to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74babe18-758b-47f6-9b19-855eb974c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfig(basis_dir+\"notebook_figures/u_net_architecture_3.png\",320)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97241f7-221a-4719-a4ad-bc325f4806de",
   "metadata": {},
   "source": [
    "### 8.1 Download of Zenodo skyrmion U-Net repository\n",
    "\n",
    "Comment this out only if you are going to train the large U-Net. Approximately 1GB of data will be downloaded for this purpose. The Zenodo repository by Winkler et al. (2024) [https://zenodo.org/records/10997175](https://zenodo.org/records/10997175) will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7a06b-eb41-4da9-9844-044a46bcb593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import wget\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "zenodo_folder = basis_dir+\"zenodo_dataset/\"\n",
    "if not os.path.exists(zenodo_folder):\n",
    "    os.makedirs(zenodo_folder)\n",
    "\n",
    "    wget.download(\"https://zenodo.org/records/10997175/files/public_unet_skyrmion_dataset.zip\",zenodo_folder+\"zenodo_unet_skyrmion_dataset.zip\")\n",
    "    with zipfile.ZipFile(zenodo_folder+\"zenodo_unet_skyrmion_dataset.zip\",\"r\") as zip:\n",
    "        zip.extractall(zenodo_folder)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e645c-2d78-43ec-bcee-ac6da4e130fd",
   "metadata": {},
   "source": [
    "### 8.2 Load dataset and split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68743fa7-cc18-43b4-bde9-f903124ef50a",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a715b3d-8c26-4619-a960-eb7a3d204d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "zenodo_dataset_folder = basis_dir+\"zenodo_dataset/public_unet_skyrmion_dataset/\"\n",
    "zenodo_dataset = pd.read_csv(zenodo_dataset_folder+\"table.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb1e3d-7170-4b77-99a6-405a7031d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfnimg = sorted(list(glob.iglob(zenodo_dataset_folder+\"images/*.png\")))\n",
    "lfnlabel = sorted(list(glob.iglob(zenodo_dataset_folder+\"labels/*.png\")))\n",
    "limg = np.array([np.array(Image.open(ele)) for ele in lfnimg])\n",
    "llabel = np.array([trafo_rgb_to_class(np.array(Image.open(ele))) for ele in lfnlabel])\n",
    "img_size = limg.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9a1a4-1b05-4e4f-8d81-77dad64e6e51",
   "metadata": {},
   "source": [
    "Split dataset in training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da269a-536f-437f-9c41-86deb75de598",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(zenodo_dataset_folder+\"partition.txt\",\"r\") as f:\n",
    "    S = f.read()\n",
    "traintest_batch = []\n",
    "trainonly_batch = []\n",
    "for ele in S.split(\"\\n\")[:-1]:\n",
    "    lt = ele.split(\";\")\n",
    "    if lt[0]==\"train_test_val\":\n",
    "        traintest_batch.append([int(ele) for ele in lt[1:]])\n",
    "    elif lt[0]==\"only_training\":\n",
    "        trainonly_batch.append([int(ele) for ele in lt[1:]])\n",
    "    else:\n",
    "        print(\"error\")\n",
    "\n",
    "\n",
    "#Association of batches to training, validation and testing\n",
    "train_batch,val_batch,test_batch = [0,1,4,5],[2],[3]\n",
    "#Assignment of indices for each training object (image and label) to validation, test, and training\n",
    "train_sources = [ele1 for ele in trainonly_batch for ele1 in ele]+[ele for ix in train_batch for ele in traintest_batch[ix]]\n",
    "train_ix = np.array(sorted([ele1 for ele in train_sources for ele1 in zenodo_dataset[zenodo_dataset[\"source_id\"]==ele].index]))\n",
    "val_sources = [ele for ix in val_batch for ele in traintest_batch[ix]]\n",
    "val_ix = np.array(sorted([ele1 for ele in val_sources for ele1 in zenodo_dataset[zenodo_dataset[\"source_id\"]==ele].index]))\n",
    "test_sources = [ele for ix in test_batch for ele in traintest_batch[ix]]\n",
    "test_ix = np.array(sorted([ele1 for ele in test_sources for ele1 in zenodo_dataset[zenodo_dataset[\"source_id\"]==ele].index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1c09a-b8d5-4e31-9fa7-baa7b02e817d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 8.3 Augmentation & Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c1a17-570b-48ab-8da3-6f23e03492f1",
   "metadata": {},
   "source": [
    "Now define the Augmentation and Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f293668-afb3-42e5-ae4c-fec4c0f48797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size refers to how many images will be passed through the U-Net at the same time during training\n",
    "#One must adjust the batch_size based on available RAM/VRAM.\n",
    "batch_size = 5\n",
    "    \n",
    "#stronger augmentation, compared to the mini U-Net\n",
    "\n",
    "def get_augmentation():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.GaussNoise(p=1,std_range=(0,25/255)),\n",
    "        albumentations.Affine(translate_percent=0.5,   scale=(0.85, 1.4),  rotate=(-90, 90),p=1,keep_ratio=True,border_mode=cv2.BORDER_REFLECT_101),\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=0.3,contrast_limit=0.5,p=1)], p=1)\n",
    "\n",
    "\n",
    "basis = np.eye(3,dtype=np.uint8)\n",
    "#creation of data generators for train and test\n",
    "dg_train = DataGenerator(limg[train_ix],basis[llabel[train_ix]],batch_size,smoothing=False,aug=get_augmentation())\n",
    "dg_val = DataGenerator(limg[val_ix],basis[llabel[val_ix]],batch_size,smoothing=False,aug=get_augmentation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b746b5c2-4842-4d8b-99de-2aa5476774b3",
   "metadata": {},
   "source": [
    "### 8.4 Defining the large U-Net architecture\n",
    "\n",
    "Define the large U-Net architecture based on the architecture plotted at the beginning of this Section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3cf99-34c5-4a1f-8751-d27d8d8b5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeln = \"model_large\"\n",
    "folder = basis_dir+\"training/\"\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "modelfn = folder+modeln+\".keras\"\n",
    "\n",
    "model = get_unet({\"name\":\"unet\",\"input_shape\": img_size, \"n_class\":3,\"filter_multiplier\":16,\"n_depth\":4,\n",
    "                  \"kernel_initialization\":\"he_normal\",\"dropout\":0.1,\"kernel_size\":(3,3),\"upsample_channel_multiplier\":8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e764c18-d377-4df6-932e-bf3fba41494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693040c-cd3a-41d3-8598-bd0c53ade9da",
   "metadata": {},
   "source": [
    "### 8.5 Training\n",
    "\n",
    "Now we train the last model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7deb96-6bbd-42cb-9b07-54b20d228784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=get_cross_entropy_loss([6.2,10,1.2]),metrics=[\"accuracy\",MCC(0)])\n",
    "history = model.fit(x=dg_train,validation_data=dg_val,epochs=15,verbose=1,batch_size=batch_size,callbacks=[\n",
    "tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',mode='min',factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
    "tf.keras.callbacks.ModelCheckpoint(modelfn, verbose=1,monitor='val_loss',mode='min',save_best_only=True,save_weights_only=False)])\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)  \n",
    "hist_df.to_csv(folder+modeln+\"_history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033c5fe-8144-4826-8885-e7715546fff1",
   "metadata": {},
   "source": [
    "Example plot of a prediction from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504aee43-4dd0-4c33-a192-ddffd2584952",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,figsize=(10,4),dpi=300)\n",
    "ix0 = 800\n",
    "ax[0].imshow(limg[ix0],cmap=\"gray\")\n",
    "ax[1].imshow(trafo_class_to_rgb(llabel[ix0]))\n",
    "ax[2].imshow(trafo_class_to_rgb(predict(np.array([limg[ix0]]),modelfn,5)[0]))\n",
    "ax[0].set_title(\"Kerr image\")\n",
    "ax[1].set_title(\"Ground truth\")\n",
    "ax[2].set_title(\"Predicted label\")\n",
    "\n",
    "for i in [0,1,2]:\n",
    "    cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3],3), \n",
    "                        cmap=matplotlib.colors.ListedColormap([(0,0,1),(0,1,0),(1,0,0)])),\n",
    "                        ax=ax[i], ticks=[0.5, 1.5, 2.5],orientation='horizontal')\n",
    "    cbar.set_ticklabels(['Background', 'Defects', 'Skyrmions'])\n",
    "    if i == 0:\n",
    "        cbar.ax.set_visible(False)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91595cc9-f5aa-4392-9167-b6a165ac4fb8",
   "metadata": {},
   "source": [
    "Benchmark now the large Skyrmion U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641f6db-f816-4c91-96db-f3027539a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pixelwise Matthews correlation coefficient on test set (true=skyrmion,false={{defect,background}}) \\n= {batch_mcc(limg[test_ix],llabel[test_ix],modelfn,0,batch_size):.3f}\")\n",
    "print(f\"Pixelwise Matthews correlation coefficient  on complete set (true=skyrmion,false={{defect,background}}) \\n= {batch_mcc(limg,llabel,modelfn,0,batch_size):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a2bd8-7f69-469b-8ab9-ddf50ed0b05a",
   "metadata": {},
   "source": [
    "## 9. MDAI (Magnetic Data Aggregator for AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c8a65-3f2b-422b-b97a-5f290a943733",
   "metadata": {},
   "source": [
    "How can one obtain data from magnetic systems to train their own AI model? → Magnetic Data Aggregator for AI (MDAI) later in the session.\n",
    "\n",
    "\n",
    "The discussed MDAI — Magnetic Data Aggregator for AI — repository can be found at: [https://github.com/kfjml/MDAI](https://github.com/kfjml/MDAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c8f97-7ad7-436a-b8d7-9d5607da7b86",
   "metadata": {},
   "source": [
    "## 10. Additional information\n",
    "\n",
    "Further information on the Skyrmion U-Net can be found in the paper: Labrie-Boulay et al., *Phys. Rev. Applied* **21**, 014014 (2023). The complete training data and models (the models are also included here in this repository) can be found in the Zenodo repository by Winkler et al. [https://zenodo.org/records/10997175](https://zenodo.org/records/10997175) (2024), which is used for training the large Skyrmion-Net in Section 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091e63d-71e5-45e7-8b0f-3b265a63cded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
