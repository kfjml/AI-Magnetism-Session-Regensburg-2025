{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e92294-d292-4371-a0e5-a1c405a67722",
   "metadata": {},
   "source": [
    "# Notebook 2) Skyrmion U-Net prediction / inference\n",
    "\n",
    "This notebook demonstrates how to perform prediction/inference using existing Skyrmion U-Net models. It can also be used later for research. A GUI for prediction and analysis can be found in `3_Editor.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83c7bf-7b3a-46bb-acde-5fa732b43e60",
   "metadata": {},
   "source": [
    "## 0. Configure notebook & import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f11eec-d152-48fb-85f0-76e0e27b13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "    ![ ! -d \"AI-Magnetism-Session-Regensburg-2025\" ] ||  [ ! -d \"AI-Magnetism-Session-Regensburg-2025/.git\" ] && git clone https://github.com/kfjml/AI-Magnetism-Session-Regensburg-2025\n",
    "    ! pip install \"numba>=0.61.0,<0.62\" \"tensorflow[and-cuda]>=2.16.2,<3\" \"albumentations>=2.0.4,<3\"  \"pandas>=2.2.2,<3\" \"chardet>=5.2.0,<6\" \"opencv-python-headless>=4.11.0.86,<5\" \"wget>=3.2,<4\" \"pyyaml>=6.0.2, <7\" \"pillow>=11.1.0, <12\"\n",
    "    ! pip install \"ipympl>=0.9.6\" \"ipywidgets>=7.7.1\" \"matplotlib>=3.10.0,<4\"\n",
    "    basis_dir = \"/content/AI-Magnetism-Session-Regensburg-2025/\"\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except:\n",
    "    basis_dir = \"./\"    \n",
    "    in_colab = False\n",
    "    \n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.spatial\n",
    "import glob\n",
    "import io\n",
    "import pandas as pd\n",
    "import ipywidgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76621558-30ee-4d5f-9392-17a55c3bf700",
   "metadata": {},
   "source": [
    "### If you are running this notebook in **Google Colab**, after executing the first cell (cell above), go to **Runtime → Restart session**, then rerun the first cell. After that, you can execute the cells below. This is necessary because some required packages are installed in Google Colab and need a restart to take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e40b2-6d56-4d17-a0b2-40e0a8e5a1dc",
   "metadata": {},
   "source": [
    "### Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650175c-7387-47aa-a5f6-c7777d5864f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_available = lambda : len(tf.config.list_physical_devices('GPU'))\n",
    "if gpu_available(): print(\"GPU is available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44317847-0260-4e5d-bd37-61714d303482",
   "metadata": {},
   "source": [
    "## 1. U-Net Architecture of the Trained and Available Models\n",
    "\n",
    "The U-Net models in this repository were trained to predict on 512x512 Kerr microscopy images.\n",
    "\n",
    "We reuse in this section code from the first notebook. For an explanation of the code, please refer to `1_Training_tutorial.ipynb`.\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ee12e-4b63-47cb-acdc-23e69638c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define plot function for figures\n",
    "def plotfig(fn,dpi):\n",
    "    fig,ax = plt.subplots(dpi=dpi)\n",
    "    ax.imshow(plt.imread(fn))\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "plotfig(basis_dir+\"notebook_figures/u_net_architecture_1.png\",420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87451f-4cf8-4305-8430-a95b2ecfb1b3",
   "metadata": {},
   "source": [
    "### 1.2 Define U-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f66a8-8aca-48a0-a55d-c2fa104cc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic activation layer\n",
    "class MishLayer(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.keras.activations.mish(x)\n",
    "\n",
    "# Basic Convolution Block\n",
    "def conv_block(x, n_channels, param):\n",
    "    x = tf.keras.layers.Conv2D(n_channels, kernel_size=param[\"kernel_size\"],kernel_initializer=param[\"kernel_initialization\"],padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x) \n",
    "    x = MishLayer()(x)\n",
    "    return x\n",
    "\n",
    "# Double Convolution Block used in \"encoder\" and \"bottleneck\"\n",
    "def double_conv_block(x, n_channels, param):\n",
    "    x = conv_block(x,n_channels,param)\n",
    "    x = conv_block(x,n_channels,param)\n",
    "    return x\n",
    "\n",
    "# Downsample block for feature extraction (encoder)\n",
    "def downsample_block(x, n_channels, param):\n",
    "    f = double_conv_block(x, n_channels, param)\n",
    "    p = tf.keras.layers.MaxPool2D(pool_size=(2,2))(f)\n",
    "    p = tf.keras.layers.Dropout(param[\"dropout\"])(p)\n",
    "    return f, p\n",
    "\n",
    "# Upsample block for the decoder\n",
    "def upsample_block(x, conv_features, n_channels, param):\n",
    "    x = tf.keras.layers.Conv2DTranspose(n_channels*param[\"upsample_channel_multiplier\"], param[\"kernel_size\"], strides=(2,2), padding='same')(x)\n",
    "    x = tf.keras.layers.concatenate([x, conv_features])\n",
    "    x = tf.keras.layers.Dropout(param[\"dropout\"])(x)\n",
    "    x = double_conv_block(x, n_channels, param)\n",
    "    return x\n",
    "\n",
    "# Create the model\n",
    "def get_unet(param):\n",
    "    input = tf.keras.layers.Input(shape=param[\"input_shape\"]+(1,))\n",
    "    next_input = input\n",
    "    \n",
    "    l_residual_con = []\n",
    "    for i in range(param[\"n_depth\"]):\n",
    "        residual_con,next_input = downsample_block(next_input, (2**i)*param[\"filter_multiplier\"],param)\n",
    "        l_residual_con.append(residual_con)\n",
    "\n",
    "    next_input = double_conv_block(next_input, (2**param[\"n_depth\"])*param[\"filter_multiplier\"],param)\n",
    "\n",
    "    for i in range(param[\"n_depth\"]):\n",
    "        next_input = upsample_block(next_input, l_residual_con[param[\"n_depth\"]-1-i], (2**(param[\"n_depth\"]-1-i))*param[\"filter_multiplier\"],param)\n",
    "\n",
    "    output = tf.keras.layers.Conv2D(param[\"n_class\"], (1,1), padding=\"same\", activation = \"softmax\",dtype='float32')(next_input)    \n",
    "    \n",
    "    return tf.keras.Model(input, output, name=param[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dde9c1-a0f6-4e22-a03b-e65ed668fe9d",
   "metadata": {},
   "source": [
    "### 1.3 Define Segmentation Mask Index ↔ RGB Conversion Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755beba1-3f47-43df-b1ec-f641019a5c54",
   "metadata": {},
   "source": [
    "The Kerr micrographs are labeled with a segmentation mask. The segmentation mask in the dataset consists of five distinct classes:\n",
    "\n",
    "- **Skyrmions** — RGB label: red [1, 0, 0]  \n",
    "- **Defects** — RGB label: green [0, 1, 0]  \n",
    "- **Ferromagnetic (FM) background** — RGB label: blue [0, 0, 1]  \n",
    "- **Non-Ferromagnetic (FM) background** — RGB label: yellow [1, 1, 0]  \n",
    "- **Boundary non-Ferromagnetic/Ferromagnetic background** — RGB label: cyan [0, 1, 1]  \n",
    "\n",
    "\n",
    "The **3-class U-Net** model predicts:\n",
    "\n",
    "- **Skyrmions** - RGB label: red [255, 0, 0]  \n",
    "- **Defects** - RGB label: green [0, 255, 0]\n",
    "- **Background** - RGB label: blue [0, 0, 255]  \n",
    "   - The background class includes:\n",
    "     - The ferromagnetic (FM) background  \n",
    "     - The non-ferromagnetic (non-FM) background  \n",
    "     - The boundary between ferromagnetic and non-ferromagnetic backgrounds\n",
    "\n",
    "For the **2023 model**, the class indices are:  \n",
    "\n",
    "- **Skyrmions:** 0  \n",
    "- **Defects:** 2\n",
    "- **Background:** 1  \n",
    "\n",
    "For the **2022 model**, the class indices are:  \n",
    "\n",
    "- **Skyrmions:** 0  \n",
    "- **Background:** 1  \n",
    "- **Defects:** 2\n",
    "\n",
    "For the **2022 inversion model**, the class indices are:  \n",
    "\n",
    "- **Skyrmions:** 0  \n",
    "- **Defects:** 1  \n",
    "- **Background:** 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a425362-e525-440e-9276-7d02e1b4ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trafo_channel_to_rgb(I):\n",
    "    basis = np.array([[255,0,0],[0,255,0],[0,0,255]],dtype=np.uint8)\n",
    "    return basis[I]\n",
    "\n",
    "def trafo_rgb_to_channel(I):\n",
    "    Q = np.zeros((I.shape[0],I.shape[1]),dtype=np.uint8)\n",
    "    R,G,B = I[:,:,0],I[:,:,1],I[:,:,2]\n",
    "    skyrmion_mask = (R>=128)&(G<128)&(B<128)\n",
    "    defect_mask = (R<128)&(G>=128)&(B<128)\n",
    "    bck_mask = ~(skyrmion_mask|defect_mask)\n",
    "    Q[skyrmion_mask] = 0\n",
    "    Q[defect_mask] = 1\n",
    "    Q[bck_mask] = 2\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb1c0b-b42e-46a8-8de1-22ab572ae1b1",
   "metadata": {},
   "source": [
    "### 1.4 Define Matthews correlation coefficient (MCC) Code\n",
    "\n",
    "For an explanation, please refer to the notebook `1_Training_and_prediction_tutorial.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74439850-77fe-43ef-8319-ded7c70b3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TF_PN(y_true,y_pred,ix0):\n",
    "    m1,m2 = y_true==ix0,y_pred==ix0\n",
    "    im1,im2 = tf.math.logical_not(m1),tf.math.logical_not(m2)\n",
    "    TP = tf.math.reduce_mean(tf.cast(tf.math.logical_and(m1,m2),dtype=np.float64))\n",
    "    TN = tf.math.reduce_mean(tf.cast(tf.math.logical_and(im1,im2),dtype=np.float64))\n",
    "    FP = tf.math.reduce_mean(tf.cast(tf.math.logical_and(im1,m2),dtype=np.float64))\n",
    "    FN = tf.math.reduce_mean(tf.cast(tf.math.logical_and(m1,im2),dtype=np.float64))\n",
    "    return TP,TN,FP,FN\n",
    "\n",
    "def get_mcc_from_TF_PN(TP,TN,FP,FN):\n",
    "    denom = tf.keras.ops.sqrt((TP + FN) * (FP + TN) * (FP + TP) * (FN + TN))\n",
    "    val = (TP * TN - FP * FN) / denom\n",
    "    return  tf.where(tf.equal(denom, 0), tf.constant(0, dtype=tf.float64), val)\n",
    "\n",
    "def get_mcc(y_true,y_false,n_class):\n",
    "    return get_mcc_from_TF_PN(*get_TF_PN(y_true,y_false,n_class)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c20477-9d0e-4911-9113-5f8b570c05de",
   "metadata": {},
   "source": [
    "## 2. Function for Prediction with U-Net model\n",
    "\n",
    "Once again, as in the other notebook, here is a function for prediction with the U-Net model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb147800-25f8-4452-bb44-994e081f5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the label based on Kerr images and the U-Net model.\n",
    "def predict(x,fn_model,batch_size=5,normalize_255=True):\n",
    "    #load U-Net model\n",
    "    model = tf.keras.models.load_model(fn_model,compile=False,custom_objects={'MishLayer': MishLayer})\n",
    "    \n",
    "    if not gpu_available():\n",
    "        #create identical model, only with pure float_32 policy\n",
    "        batch_size = 1\n",
    "        nmodel = get_unet({\"name\":\"unet\",\"input_shape\": (512,512), \"n_class\":3,\"filter_multiplier\":16,\"n_depth\":4,\n",
    "                  \"kernel_initialization\":\"he_normal\",\"dropout\":0.1,\"kernel_size\":(3,3),\"upsample_channel_multiplier\":8})\n",
    "        nmodel.set_weights(model.weights)\n",
    "        model = nmodel\n",
    "    \n",
    "    n = int(np.ceil(len(x)/batch_size))\n",
    "    lix = [np.array(range(j*batch_size,min((j+1)*batch_size,len(x)))) for j in range(n)]\n",
    "    ylabel = np.zeros(x.shape,dtype=np.uint8)\n",
    "    progbar = tf.keras.utils.Progbar(n)\n",
    "    for i in range(n):            \n",
    "        progbar.update(i)\n",
    "        input = x[lix[i]]\n",
    "        if normalize_255:\n",
    "            input = input/255\n",
    "        ylabel[lix[i]] = model.predict(input,verbose=False).argmax(-1)\n",
    "    progbar.update(n,finalize=True)\n",
    "    return ylabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d59683-b1ea-4ca4-82bd-f18e04fa2c4e",
   "metadata": {},
   "source": [
    "## 3. Make Predictions Using Data Available in This Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00083b-01c7-4423-85c0-a05474f26541",
   "metadata": {},
   "source": [
    "Load filenames of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ae334-8ebe-4485-b38a-8c93542e4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(basis_dir+\"dataset/table.csv\",sep=\";\")\n",
    "dataset[\"img_fn\"] = dataset.apply(lambda x:basis_dir+x[\"img_fn\"],axis=1)\n",
    "dataset[\"label_fn\"] = dataset.apply(lambda x:basis_dir+x[\"label_fn\"],axis=1)\n",
    "\n",
    "fnimg,fnlabel = list(dataset.img_fn.to_numpy()),list(dataset.label_fn.to_numpy())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a661a-627d-4b9b-919f-cd2d5453d485",
   "metadata": {},
   "source": [
    "In the following, we will explored the pretrained models in this repository using example predictions. The models are trained on an input size of **512x512 pixels**, so we will crop the images accordingly in the following steps. \n",
    "\n",
    "For certain (also much larger) input sizes, the U-Net model can simply be redefined, the trained weights imported, and the prediction will still work on larger images.  If a specific pixel size is not supported, the image can be divided into tiles. This is not entirely straightforward, as there are better and worse ways to define the tiles for prediction. After performing predictions on the tiles, they can be stitched back together.  This method is also implemented in the next notebook: `3_Editor.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c784b3-c6e1-49a5-9936-493c371cdd42",
   "metadata": {},
   "source": [
    "### 3.1 Model 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b20ad7-10e7-4236-8e91-87aec23d3f9c",
   "metadata": {},
   "source": [
    "Now we make a prediction with the model 2023. The variable **ix** can be changed to predict different images from this dataset repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4040a-8fe1-4378-8891-208cb53ed2c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load imag and label\n",
    "ix = 129\n",
    "img = np.array(Image.open(fnimg[ix]))\n",
    "label = np.array(Image.open(fnlabel[ix]))\n",
    "#cut to 512x512\n",
    "img,label = img[:512,:512],label[:512,:512]\n",
    "#Predict label with U-Net\n",
    "predicted_label = predict(np.array([img]),basis_dir+\"models/2023_model.keras\",batch_size=1)[0]\n",
    "#Swap class index of 1 and 2, since for model 2023 the class indeces are (skyrmion:0, background:1, defects:2) and the functions are written for class indeces (skyrmion:0, defects:1, background:2)\n",
    "predicted_label[predicted_label==1] = 9\n",
    "predicted_label[predicted_label==2] = 1\n",
    "predicted_label[predicted_label==9] = 2\n",
    "#Evaluate predicitionn with Matthews correlation coefficient\n",
    "print(\"Pixelwise Matthews correlation coefficient (true=skyrmion,false=defect,background)\",get_mcc(trafo_rgb_to_channel(label),predicted_label,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd521313-16b8-42f6-9d3d-f5373e3db1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "%matplotlib inline\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3,dpi=300,figsize=(15,3.5))\n",
    "ax[0].imshow(img,cmap=\"gray\")\n",
    "ax[1].imshow(label)\n",
    "ax[2].imshow(trafo_channel_to_rgb(predicted_label))\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3],3), \n",
    "                        cmap=matplotlib.colors.ListedColormap([(0,0,1),(0,1,0),(1,0,0)])),\n",
    "                        ax=ax[2], ticks=[0.5, 1.5, 2.5])\n",
    "cbar.set_ticklabels(['Background', 'Defects', 'Skyrmions'])\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3,4,5],6), \n",
    "                    cmap=matplotlib.colors.ListedColormap([(0, 1, 1), (1, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0)])),\n",
    "                    ax=ax[1], ticks=[0.5, 1.5, 2.5,3.5,4.5])\n",
    "cbar.set_ticklabels(['FM-non FM Boundary', 'non FM Bakckground', 'FM Bakckground', 'Defects', 'Skyrmions'])\n",
    "\n",
    "ax[0].set_title(\"Kerr image\")\n",
    "ax[1].set_title(\"Ground truth\")\n",
    "ax[2].set_title(\"Predicted label\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c3496-587f-4866-b31f-dc0520c1af57",
   "metadata": {},
   "source": [
    "### 3.2 Model 2022\n",
    "Now we will try another model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea955e65-e7ac-49be-a4ef-5c5638856d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load imag and label\n",
    "ix = 78\n",
    "img = np.array(Image.open(fnimg[ix]))\n",
    "label = np.array(Image.open(fnlabel[ix]))\n",
    "#cut to 512x512\n",
    "img,label = img[:512,:512],label[:512,:512]\n",
    "#Predict label with U-Net\n",
    "predicted_label = predict(np.array([img]),basis_dir+\"models/2022_model.keras\",batch_size=1)[0]\n",
    "#Evaluate predicitionn with Matthews correlation coefficient\n",
    "print(\"Pixelwise Matthews correlation coefficient (true=skyrmion,false=defect,background)\",get_mcc(trafo_rgb_to_channel(label),predicted_label,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa7d6f-e369-48fd-85f8-09e61b69ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "%matplotlib inline\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3,dpi=300,figsize=(15,3.5))\n",
    "ax[0].imshow(img,cmap=\"gray\")\n",
    "ax[1].imshow(label)\n",
    "ax[2].imshow(trafo_channel_to_rgb(predicted_label))\n",
    "ax[0].set_title(\"Kerr image\")\n",
    "ax[1].set_title(\"Ground truth\")\n",
    "ax[2].set_title(\"Predicted label\")\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3],3), \n",
    "                        cmap=matplotlib.colors.ListedColormap([(0,0,1),(0,1,0),(1,0,0)])),\n",
    "                        ax=ax[2], ticks=[0.5, 1.5, 2.5])\n",
    "cbar.set_ticklabels(['Background', 'Defects', 'Skyrmions'])\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3,4,5],6), \n",
    "                    cmap=matplotlib.colors.ListedColormap([(0, 1, 1), (1, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0)])),\n",
    "                    ax=ax[1], ticks=[0.5, 1.5, 2.5,3.5,4.5])\n",
    "cbar.set_ticklabels(['FM-non FM Boundary', 'non FM Bakckground', 'FM Bakckground', 'Defects', 'Skyrmions'])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f99ee-769e-458d-90fa-28873ad6cd7c",
   "metadata": {},
   "source": [
    "### 3.3 Model Inversion 2022\n",
    "\n",
    "And now, Model Inversion 2022, which also works with Kerr micrographs featuring both normal and inverted intensity (bright skyrmions on a dark background):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8b647-99e3-4db7-8991-e2fd148cf7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load imag and label\n",
    "ix = 1\n",
    "img = 255-np.array(Image.open(fnimg[ix]))\n",
    "label = np.array(Image.open(fnlabel[ix]))\n",
    "#cut to 512x512\n",
    "img,label = img[:512,:512],label[:512,:512]\n",
    "#Predict label with U-Net\n",
    "predicted_label = predict(np.array([img]),basis_dir+\"models/2022_model_inv.keras\",batch_size=1)[0]\n",
    "#Evaluate predicitionn with Matthews correlation coefficient\n",
    "print(\"Pixelwise Matthews correlation coefficient (true=skyrmion,false=defect,background)\",get_mcc(trafo_rgb_to_channel(label),predicted_label,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c14daf-6f25-4859-8911-a99f4f02db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "%matplotlib inline\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3,dpi=300,figsize=(15,3.5))\n",
    "ax[0].imshow(img,cmap=\"gray\")\n",
    "ax[1].imshow(label)\n",
    "ax[2].imshow(trafo_channel_to_rgb(predicted_label))\n",
    "ax[0].set_title(\"Kerr image\")\n",
    "ax[1].set_title(\"Ground truth\")\n",
    "ax[2].set_title(\"Predicted label\")\n",
    "\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3],3), \n",
    "                        cmap=matplotlib.colors.ListedColormap([(0,0,1),(0,1,0),(1,0,0)])),\n",
    "                        ax=ax[2], ticks=[0.5, 1.5, 2.5])\n",
    "cbar.set_ticklabels(['Background', 'Defects', 'Skyrmions'])\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.BoundaryNorm([0,1,2,3,4,5],6), \n",
    "                    cmap=matplotlib.colors.ListedColormap([(0, 1, 1), (1, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0)])),\n",
    "                    ax=ax[1], ticks=[0.5, 1.5, 2.5,3.5,4.5])\n",
    "cbar.set_ticklabels(['FM-non FM Boundary', 'non FM Bakckground', 'FM Bakckground', 'Defects', 'Skyrmions'])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df18028-f99b-4c7b-957b-fec223bf822a",
   "metadata": {},
   "source": [
    "## 4. Additional information\n",
    "\n",
    "Further information on the Skyrmion U-Net can be found in the paper: Labrie-Boulay et al., *Phys. Rev. Applied* **21**, 014014 (2023). The complete training data and models (the models are also included here in this repository) can be found in the Zenodo repository by Winkler et al. [https://zenodo.org/records/10997175](https://zenodo.org/records/10997175) (2024)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
